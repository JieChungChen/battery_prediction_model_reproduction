{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from random import randint\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader\n",
    "from data_preprocessing import Predictor1_Dataset\n",
    "from discharge_model import Predictor_1\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Predictor1 training', add_help=False)\n",
    "    parser.add_argument('--batch_size', default=32, type=int)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "\n",
    "    # Model parameters\n",
    "    parser.add_argument('--model_name', default='Predictor_1', type=str) \n",
    "    parser.add_argument('--finetune', default=False, type=bool)   \n",
    "    parser.add_argument('--load_checkpoint', default='predictor1_best_model.pth', type=str)                  \n",
    "\n",
    "    # Optimizer parameters\n",
    "    parser.add_argument('--weight_decay', type=float, default=5)\n",
    "    parser.add_argument('--lr', type=float, default=1e-4, metavar='LR')\n",
    "    parser.add_argument('--lr_schedule', type=bool, default=False, metavar='LR')\n",
    "    parser.add_argument('--min_lr', type=float, default=1e-5, metavar='LR')\n",
    "    parser.add_argument('--warm_up', type=int, default=10, metavar='LR')\n",
    "    parser.add_argument('--delta', type=int, default=1)\n",
    "    return parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -- GPU is available -- \n",
      "92 23\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1              [-1, 64, 100]           5,696\n",
      "              Mish-2              [-1, 64, 100]               0\n",
      "            Conv1d-3             [-1, 128, 100]          57,472\n",
      "              Mish-4             [-1, 128, 100]               0\n",
      "            Conv1d-5             [-1, 256, 100]         164,096\n",
      "              Mish-6             [-1, 256, 100]               0\n",
      "    SpatialDropout-7             [-1, 256, 100]               0\n",
      "         AvgPool1d-8              [-1, 256, 50]               0\n",
      "            Conv1d-9               [-1, 64, 50]         180,288\n",
      "             Mish-10               [-1, 64, 50]               0\n",
      "           Conv1d-11               [-1, 64, 50]         114,752\n",
      "             Mish-12               [-1, 64, 50]               0\n",
      "          Sigmoid-13               [-1, 50, 50]               0\n",
      "AdaptiveAvgPool1d-14                [-1, 64, 1]               0\n",
      "AdaptiveAvgPool1d-15                [-1, 50, 1]               0\n",
      "           Conv1d-16              [-1, 64, 106]             640\n",
      "             Mish-17              [-1, 64, 106]               0\n",
      "           Conv1d-18              [-1, 32, 100]          14,368\n",
      "             Mish-19              [-1, 32, 100]               0\n",
      "           Conv1d-20              [-1, 128, 94]          28,800\n",
      "             Mish-21              [-1, 128, 94]               0\n",
      "AdaptiveMaxPool1d-22               [-1, 128, 1]               0\n",
      "AdaptiveAvgPool1d-23               [-1, 128, 1]               0\n",
      "           Conv1d-24             [-1, 128, 108]           1,024\n",
      "             Mish-25             [-1, 128, 108]               0\n",
      "           Conv1d-26              [-1, 256, 98]         360,704\n",
      "             Mish-27              [-1, 256, 98]               0\n",
      "           Conv1d-28              [-1, 256, 96]         196,864\n",
      "             Mish-29              [-1, 256, 96]               0\n",
      "AdaptiveMaxPool1d-30               [-1, 256, 1]               0\n",
      "AdaptiveAvgPool1d-31               [-1, 256, 1]               0\n",
      "           Linear-32                    [-1, 1]             129\n",
      "           Linear-33                    [-1, 1]             257\n",
      "================================================================\n",
      "Total params: 1,125,090\n",
      "Trainable params: 1,125,090\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 2.40\n",
      "Params size (MB): 4.29\n",
      "Estimated Total Size (MB): 6.70\n",
      "----------------------------------------------------------------\n",
      "epoch:[1 / 50] batch:[1 / 287] loss= 0.751 lr= 1.00e-04\n",
      "epoch:[1 / 50] batch:[51 / 287] loss= 0.427 lr= 1.00e-04\n",
      "epoch:[1 / 50] batch:[101 / 287] loss= 0.362 lr= 1.00e-04\n",
      "epoch:[1 / 50] batch:[151 / 287] loss= 0.259 lr= 1.00e-04\n",
      "epoch:[1 / 50] batch:[201 / 287] loss= 0.390 lr= 1.00e-04\n",
      "epoch:[1 / 50] batch:[251 / 287] loss= 0.258 lr= 1.00e-04\n",
      "trn_loss: 0.129, val_loss: 0.271\n",
      "training set RMSE 1 cycle: 127, 20 cycle: 121, 100 cycle: 112\n",
      "testing set RMSE 1 cycle: 186, 20 cycle: 189, 100 cycle: 162\n",
      "epoch:[2 / 50] batch:[1 / 287] loss= 0.261 lr= 1.00e-04\n",
      "epoch:[2 / 50] batch:[51 / 287] loss= 0.177 lr= 1.00e-04\n",
      "epoch:[2 / 50] batch:[101 / 287] loss= 0.205 lr= 1.00e-04\n",
      "epoch:[2 / 50] batch:[151 / 287] loss= 0.357 lr= 1.00e-04\n",
      "epoch:[2 / 50] batch:[201 / 287] loss= 0.366 lr= 1.00e-04\n",
      "epoch:[2 / 50] batch:[251 / 287] loss= 0.253 lr= 1.00e-04\n",
      "trn_loss: 0.123, val_loss: 0.200\n",
      "training set RMSE 1 cycle: 128, 20 cycle: 122, 100 cycle: 109\n",
      "testing set RMSE 1 cycle: 167, 20 cycle: 165, 100 cycle: 139\n",
      "epoch:[3 / 50] batch:[1 / 287] loss= 0.205 lr= 1.00e-04\n",
      "epoch:[3 / 50] batch:[51 / 287] loss= 0.219 lr= 1.00e-04\n",
      "epoch:[3 / 50] batch:[101 / 287] loss= 0.236 lr= 1.00e-04\n",
      "epoch:[3 / 50] batch:[151 / 287] loss= 0.221 lr= 1.00e-04\n",
      "epoch:[3 / 50] batch:[201 / 287] loss= 0.264 lr= 1.00e-04\n",
      "epoch:[3 / 50] batch:[251 / 287] loss= 0.215 lr= 1.00e-04\n",
      "trn_loss: 0.114, val_loss: 0.170\n",
      "training set RMSE 1 cycle: 120, 20 cycle: 112, 100 cycle: 105\n",
      "testing set RMSE 1 cycle: 158, 20 cycle: 152, 100 cycle: 128\n",
      "epoch:[4 / 50] batch:[1 / 287] loss= 0.174 lr= 1.00e-04\n",
      "epoch:[4 / 50] batch:[51 / 287] loss= 0.194 lr= 1.00e-04\n",
      "epoch:[4 / 50] batch:[101 / 287] loss= 0.253 lr= 1.00e-04\n",
      "epoch:[4 / 50] batch:[151 / 287] loss= 0.157 lr= 1.00e-04\n",
      "epoch:[4 / 50] batch:[201 / 287] loss= 0.179 lr= 1.00e-04\n",
      "epoch:[4 / 50] batch:[251 / 287] loss= 0.191 lr= 1.00e-04\n",
      "trn_loss: 0.114, val_loss: 0.172\n",
      "training set RMSE 1 cycle: 125, 20 cycle: 114, 100 cycle: 105\n",
      "testing set RMSE 1 cycle: 161, 20 cycle: 149, 100 cycle: 129\n",
      "epoch:[5 / 50] batch:[1 / 287] loss= 0.372 lr= 1.00e-04\n",
      "epoch:[5 / 50] batch:[51 / 287] loss= 0.232 lr= 1.00e-04\n",
      "epoch:[5 / 50] batch:[101 / 287] loss= 0.175 lr= 1.00e-04\n",
      "epoch:[5 / 50] batch:[151 / 287] loss= 0.254 lr= 1.00e-04\n",
      "epoch:[5 / 50] batch:[201 / 287] loss= 0.206 lr= 1.00e-04\n",
      "epoch:[5 / 50] batch:[251 / 287] loss= 0.194 lr= 1.00e-04\n",
      "trn_loss: 0.096, val_loss: 0.159\n",
      "training set RMSE 1 cycle: 117, 20 cycle: 107, 100 cycle: 96\n",
      "testing set RMSE 1 cycle: 165, 20 cycle: 156, 100 cycle: 124\n",
      "epoch:[6 / 50] batch:[1 / 287] loss= 0.306 lr= 1.00e-04\n",
      "epoch:[6 / 50] batch:[51 / 287] loss= 0.186 lr= 1.00e-04\n",
      "epoch:[6 / 50] batch:[101 / 287] loss= 0.264 lr= 1.00e-04\n",
      "epoch:[6 / 50] batch:[151 / 287] loss= 0.285 lr= 1.00e-04\n",
      "epoch:[6 / 50] batch:[201 / 287] loss= 0.158 lr= 1.00e-04\n",
      "epoch:[6 / 50] batch:[251 / 287] loss= 0.236 lr= 1.00e-04\n",
      "trn_loss: 0.085, val_loss: 0.136\n",
      "training set RMSE 1 cycle: 116, 20 cycle: 105, 100 cycle: 91\n",
      "testing set RMSE 1 cycle: 146, 20 cycle: 138, 100 cycle: 115\n",
      "epoch:[7 / 50] batch:[1 / 287] loss= 0.256 lr= 1.00e-04\n",
      "epoch:[7 / 50] batch:[51 / 287] loss= 0.195 lr= 1.00e-04\n",
      "epoch:[7 / 50] batch:[101 / 287] loss= 0.214 lr= 1.00e-04\n",
      "epoch:[7 / 50] batch:[151 / 287] loss= 0.280 lr= 1.00e-04\n",
      "epoch:[7 / 50] batch:[201 / 287] loss= 0.262 lr= 1.00e-04\n",
      "epoch:[7 / 50] batch:[251 / 287] loss= 0.277 lr= 1.00e-04\n",
      "trn_loss: 0.112, val_loss: 0.158\n",
      "training set RMSE 1 cycle: 125, 20 cycle: 105, 100 cycle: 104\n",
      "testing set RMSE 1 cycle: 150, 20 cycle: 132, 100 cycle: 123\n",
      "epoch:[8 / 50] batch:[1 / 287] loss= 0.290 lr= 1.00e-04\n",
      "epoch:[8 / 50] batch:[51 / 287] loss= 0.239 lr= 1.00e-04\n",
      "epoch:[8 / 50] batch:[101 / 287] loss= 0.184 lr= 1.00e-04\n",
      "epoch:[8 / 50] batch:[151 / 287] loss= 0.237 lr= 1.00e-04\n",
      "epoch:[8 / 50] batch:[201 / 287] loss= 0.214 lr= 1.00e-04\n",
      "epoch:[8 / 50] batch:[251 / 287] loss= 0.169 lr= 1.00e-04\n",
      "trn_loss: 0.099, val_loss: 0.133\n",
      "training set RMSE 1 cycle: 125, 20 cycle: 102, 100 cycle: 98\n",
      "testing set RMSE 1 cycle: 148, 20 cycle: 128, 100 cycle: 114\n",
      "epoch:[9 / 50] batch:[1 / 287] loss= 0.213 lr= 1.00e-04\n",
      "epoch:[9 / 50] batch:[51 / 287] loss= 0.232 lr= 1.00e-04\n",
      "epoch:[9 / 50] batch:[101 / 287] loss= 0.262 lr= 1.00e-04\n",
      "epoch:[9 / 50] batch:[151 / 287] loss= 0.181 lr= 1.00e-04\n",
      "epoch:[9 / 50] batch:[201 / 287] loss= 0.154 lr= 1.00e-04\n",
      "epoch:[9 / 50] batch:[251 / 287] loss= 0.157 lr= 1.00e-04\n",
      "trn_loss: 0.076, val_loss: 0.130\n",
      "training set RMSE 1 cycle: 108, 20 cycle: 103, 100 cycle: 86\n",
      "testing set RMSE 1 cycle: 155, 20 cycle: 148, 100 cycle: 112\n",
      "epoch:[10 / 50] batch:[1 / 287] loss= 0.186 lr= 1.00e-04\n",
      "epoch:[10 / 50] batch:[51 / 287] loss= 0.216 lr= 1.00e-04\n",
      "epoch:[10 / 50] batch:[101 / 287] loss= 0.136 lr= 1.00e-04\n",
      "epoch:[10 / 50] batch:[151 / 287] loss= 0.179 lr= 1.00e-04\n",
      "epoch:[10 / 50] batch:[201 / 287] loss= 0.264 lr= 1.00e-04\n",
      "epoch:[10 / 50] batch:[251 / 287] loss= 0.193 lr= 1.00e-04\n",
      "trn_loss: 0.080, val_loss: 0.143\n",
      "training set RMSE 1 cycle: 109, 20 cycle: 108, 100 cycle: 88\n",
      "testing set RMSE 1 cycle: 165, 20 cycle: 155, 100 cycle: 118\n",
      "epoch:[11 / 50] batch:[1 / 287] loss= 0.179 lr= 1.00e-04\n",
      "epoch:[11 / 50] batch:[51 / 287] loss= 0.175 lr= 1.00e-04\n",
      "epoch:[11 / 50] batch:[101 / 287] loss= 0.177 lr= 1.00e-04\n",
      "epoch:[11 / 50] batch:[151 / 287] loss= 0.129 lr= 1.00e-04\n",
      "epoch:[11 / 50] batch:[201 / 287] loss= 0.254 lr= 1.00e-04\n",
      "epoch:[11 / 50] batch:[251 / 287] loss= 0.145 lr= 1.00e-04\n",
      "trn_loss: 0.068, val_loss: 0.139\n",
      "training set RMSE 1 cycle: 109, 20 cycle: 94, 100 cycle: 81\n",
      "testing set RMSE 1 cycle: 157, 20 cycle: 149, 100 cycle: 116\n",
      "epoch:[12 / 50] batch:[1 / 287] loss= 0.200 lr= 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[12 / 50] batch:[51 / 287] loss= 0.177 lr= 1.00e-04\n",
      "epoch:[12 / 50] batch:[101 / 287] loss= 0.162 lr= 1.00e-04\n",
      "epoch:[12 / 50] batch:[151 / 287] loss= 0.130 lr= 1.00e-04\n",
      "epoch:[12 / 50] batch:[201 / 287] loss= 0.128 lr= 1.00e-04\n",
      "epoch:[12 / 50] batch:[251 / 287] loss= 0.199 lr= 1.00e-04\n",
      "trn_loss: 0.080, val_loss: 0.135\n",
      "training set RMSE 1 cycle: 125, 20 cycle: 95, 100 cycle: 88\n",
      "testing set RMSE 1 cycle: 153, 20 cycle: 134, 100 cycle: 114\n",
      "epoch:[13 / 50] batch:[1 / 287] loss= 0.203 lr= 1.00e-04\n",
      "epoch:[13 / 50] batch:[51 / 287] loss= 0.197 lr= 1.00e-04\n",
      "epoch:[13 / 50] batch:[101 / 287] loss= 0.226 lr= 1.00e-04\n",
      "epoch:[13 / 50] batch:[151 / 287] loss= 0.188 lr= 1.00e-04\n",
      "epoch:[13 / 50] batch:[201 / 287] loss= 0.315 lr= 1.00e-04\n",
      "epoch:[13 / 50] batch:[251 / 287] loss= 0.149 lr= 1.00e-04\n",
      "trn_loss: 0.089, val_loss: 0.133\n",
      "training set RMSE 1 cycle: 124, 20 cycle: 98, 100 cycle: 93\n",
      "testing set RMSE 1 cycle: 151, 20 cycle: 133, 100 cycle: 113\n",
      "epoch:[14 / 50] batch:[1 / 287] loss= 0.170 lr= 1.00e-04\n",
      "epoch:[14 / 50] batch:[51 / 287] loss= 0.245 lr= 1.00e-04\n",
      "epoch:[14 / 50] batch:[101 / 287] loss= 0.189 lr= 1.00e-04\n",
      "epoch:[14 / 50] batch:[151 / 287] loss= 0.148 lr= 1.00e-04\n",
      "epoch:[14 / 50] batch:[201 / 287] loss= 0.194 lr= 1.00e-04\n",
      "epoch:[14 / 50] batch:[251 / 287] loss= 0.208 lr= 1.00e-04\n",
      "trn_loss: 0.064, val_loss: 0.143\n",
      "training set RMSE 1 cycle: 115, 20 cycle: 95, 100 cycle: 78\n",
      "testing set RMSE 1 cycle: 153, 20 cycle: 148, 100 cycle: 117\n",
      "epoch:[15 / 50] batch:[1 / 287] loss= 0.182 lr= 1.00e-04\n",
      "epoch:[15 / 50] batch:[51 / 287] loss= 0.187 lr= 1.00e-04\n",
      "epoch:[15 / 50] batch:[101 / 287] loss= 0.177 lr= 1.00e-04\n",
      "epoch:[15 / 50] batch:[151 / 287] loss= 0.147 lr= 1.00e-04\n",
      "epoch:[15 / 50] batch:[201 / 287] loss= 0.164 lr= 1.00e-04\n",
      "epoch:[15 / 50] batch:[251 / 287] loss= 0.135 lr= 1.00e-04\n",
      "trn_loss: 0.076, val_loss: 0.126\n",
      "training set RMSE 1 cycle: 121, 20 cycle: 94, 100 cycle: 85\n",
      "testing set RMSE 1 cycle: 146, 20 cycle: 133, 100 cycle: 110\n",
      "epoch:[16 / 50] batch:[1 / 287] loss= 0.128 lr= 1.00e-04\n",
      "epoch:[16 / 50] batch:[51 / 287] loss= 0.187 lr= 1.00e-04\n",
      "epoch:[16 / 50] batch:[101 / 287] loss= 0.155 lr= 1.00e-04\n",
      "epoch:[16 / 50] batch:[151 / 287] loss= 0.239 lr= 1.00e-04\n",
      "epoch:[16 / 50] batch:[201 / 287] loss= 0.170 lr= 1.00e-04\n",
      "epoch:[16 / 50] batch:[251 / 287] loss= 0.145 lr= 1.00e-04\n",
      "trn_loss: 0.064, val_loss: 0.151\n",
      "training set RMSE 1 cycle: 107, 20 cycle: 99, 100 cycle: 79\n",
      "testing set RMSE 1 cycle: 158, 20 cycle: 158, 100 cycle: 121\n",
      "epoch:[17 / 50] batch:[1 / 287] loss= 0.239 lr= 1.00e-04\n",
      "epoch:[17 / 50] batch:[51 / 287] loss= 0.206 lr= 1.00e-04\n",
      "epoch:[17 / 50] batch:[101 / 287] loss= 0.172 lr= 1.00e-04\n",
      "epoch:[17 / 50] batch:[151 / 287] loss= 0.130 lr= 1.00e-04\n",
      "epoch:[17 / 50] batch:[201 / 287] loss= 0.174 lr= 1.00e-04\n",
      "epoch:[17 / 50] batch:[251 / 287] loss= 0.162 lr= 1.00e-04\n",
      "trn_loss: 0.071, val_loss: 0.117\n",
      "training set RMSE 1 cycle: 117, 20 cycle: 95, 100 cycle: 83\n",
      "testing set RMSE 1 cycle: 142, 20 cycle: 138, 100 cycle: 106\n",
      "epoch:[18 / 50] batch:[1 / 287] loss= 0.136 lr= 1.00e-04\n",
      "epoch:[18 / 50] batch:[51 / 287] loss= 0.163 lr= 1.00e-04\n",
      "epoch:[18 / 50] batch:[101 / 287] loss= 0.202 lr= 1.00e-04\n",
      "epoch:[18 / 50] batch:[151 / 287] loss= 0.134 lr= 1.00e-04\n",
      "epoch:[18 / 50] batch:[201 / 287] loss= 0.207 lr= 1.00e-04\n",
      "epoch:[18 / 50] batch:[251 / 287] loss= 0.187 lr= 1.00e-04\n",
      "trn_loss: 0.065, val_loss: 0.124\n",
      "training set RMSE 1 cycle: 114, 20 cycle: 94, 100 cycle: 79\n",
      "testing set RMSE 1 cycle: 137, 20 cycle: 142, 100 cycle: 109\n",
      "epoch:[19 / 50] batch:[1 / 287] loss= 0.168 lr= 1.00e-04\n",
      "epoch:[19 / 50] batch:[51 / 287] loss= 0.141 lr= 1.00e-04\n",
      "epoch:[19 / 50] batch:[101 / 287] loss= 0.143 lr= 1.00e-04\n",
      "epoch:[19 / 50] batch:[151 / 287] loss= 0.180 lr= 1.00e-04\n",
      "epoch:[19 / 50] batch:[201 / 287] loss= 0.171 lr= 1.00e-04\n",
      "epoch:[19 / 50] batch:[251 / 287] loss= 0.131 lr= 1.00e-04\n",
      "trn_loss: 0.063, val_loss: 0.149\n",
      "training set RMSE 1 cycle: 109, 20 cycle: 100, 100 cycle: 78\n",
      "testing set RMSE 1 cycle: 150, 20 cycle: 157, 100 cycle: 120\n",
      "epoch:[20 / 50] batch:[1 / 287] loss= 0.152 lr= 1.00e-04\n",
      "epoch:[20 / 50] batch:[51 / 287] loss= 0.206 lr= 1.00e-04\n",
      "epoch:[20 / 50] batch:[101 / 287] loss= 0.115 lr= 1.00e-04\n",
      "epoch:[20 / 50] batch:[151 / 287] loss= 0.146 lr= 1.00e-04\n",
      "epoch:[20 / 50] batch:[201 / 287] loss= 0.138 lr= 1.00e-04\n",
      "epoch:[20 / 50] batch:[251 / 287] loss= 0.121 lr= 1.00e-04\n",
      "trn_loss: 0.077, val_loss: 0.121\n",
      "training set RMSE 1 cycle: 121, 20 cycle: 99, 100 cycle: 86\n",
      "testing set RMSE 1 cycle: 139, 20 cycle: 133, 100 cycle: 108\n",
      "epoch:[21 / 50] batch:[1 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[21 / 50] batch:[51 / 287] loss= 0.184 lr= 1.00e-04\n",
      "epoch:[21 / 50] batch:[101 / 287] loss= 0.133 lr= 1.00e-04\n",
      "epoch:[21 / 50] batch:[151 / 287] loss= 0.247 lr= 1.00e-04\n",
      "epoch:[21 / 50] batch:[201 / 287] loss= 0.156 lr= 1.00e-04\n",
      "epoch:[21 / 50] batch:[251 / 287] loss= 0.173 lr= 1.00e-04\n",
      "trn_loss: 0.080, val_loss: 0.111\n",
      "training set RMSE 1 cycle: 128, 20 cycle: 96, 100 cycle: 88\n",
      "testing set RMSE 1 cycle: 136, 20 cycle: 124, 100 cycle: 103\n",
      "epoch:[22 / 50] batch:[1 / 287] loss= 0.117 lr= 1.00e-04\n",
      "epoch:[22 / 50] batch:[51 / 287] loss= 0.091 lr= 1.00e-04\n",
      "epoch:[22 / 50] batch:[101 / 287] loss= 0.156 lr= 1.00e-04\n",
      "epoch:[22 / 50] batch:[151 / 287] loss= 0.170 lr= 1.00e-04\n",
      "epoch:[22 / 50] batch:[201 / 287] loss= 0.152 lr= 1.00e-04\n",
      "epoch:[22 / 50] batch:[251 / 287] loss= 0.202 lr= 1.00e-04\n",
      "trn_loss: 0.065, val_loss: 0.145\n",
      "training set RMSE 1 cycle: 112, 20 cycle: 94, 100 cycle: 79\n",
      "testing set RMSE 1 cycle: 147, 20 cycle: 146, 100 cycle: 118\n",
      "epoch:[23 / 50] batch:[1 / 287] loss= 0.122 lr= 1.00e-04\n",
      "epoch:[23 / 50] batch:[51 / 287] loss= 0.162 lr= 1.00e-04\n",
      "epoch:[23 / 50] batch:[101 / 287] loss= 0.148 lr= 1.00e-04\n",
      "epoch:[23 / 50] batch:[151 / 287] loss= 0.128 lr= 1.00e-04\n",
      "epoch:[23 / 50] batch:[201 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[23 / 50] batch:[251 / 287] loss= 0.115 lr= 1.00e-04\n",
      "trn_loss: 0.072, val_loss: 0.140\n",
      "training set RMSE 1 cycle: 123, 20 cycle: 92, 100 cycle: 83\n",
      "testing set RMSE 1 cycle: 136, 20 cycle: 138, 100 cycle: 116\n",
      "epoch:[24 / 50] batch:[1 / 287] loss= 0.188 lr= 1.00e-04\n",
      "epoch:[24 / 50] batch:[51 / 287] loss= 0.152 lr= 1.00e-04\n",
      "epoch:[24 / 50] batch:[101 / 287] loss= 0.174 lr= 1.00e-04\n",
      "epoch:[24 / 50] batch:[151 / 287] loss= 0.162 lr= 1.00e-04\n",
      "epoch:[24 / 50] batch:[201 / 287] loss= 0.198 lr= 1.00e-04\n",
      "epoch:[24 / 50] batch:[251 / 287] loss= 0.188 lr= 1.00e-04\n",
      "trn_loss: 0.064, val_loss: 0.125\n",
      "training set RMSE 1 cycle: 117, 20 cycle: 91, 100 cycle: 79\n",
      "testing set RMSE 1 cycle: 136, 20 cycle: 136, 100 cycle: 110\n",
      "epoch:[25 / 50] batch:[1 / 287] loss= 0.197 lr= 1.00e-04\n",
      "epoch:[25 / 50] batch:[51 / 287] loss= 0.165 lr= 1.00e-04\n",
      "epoch:[25 / 50] batch:[101 / 287] loss= 0.144 lr= 1.00e-04\n",
      "epoch:[25 / 50] batch:[151 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[25 / 50] batch:[201 / 287] loss= 0.107 lr= 1.00e-04\n",
      "epoch:[25 / 50] batch:[251 / 287] loss= 0.165 lr= 1.00e-04\n",
      "trn_loss: 0.064, val_loss: 0.146\n",
      "training set RMSE 1 cycle: 122, 20 cycle: 89, 100 cycle: 78\n",
      "testing set RMSE 1 cycle: 138, 20 cycle: 144, 100 cycle: 119\n",
      "epoch:[26 / 50] batch:[1 / 287] loss= 0.183 lr= 1.00e-04\n",
      "epoch:[26 / 50] batch:[51 / 287] loss= 0.126 lr= 1.00e-04\n",
      "epoch:[26 / 50] batch:[101 / 287] loss= 0.171 lr= 1.00e-04\n",
      "epoch:[26 / 50] batch:[151 / 287] loss= 0.142 lr= 1.00e-04\n",
      "epoch:[26 / 50] batch:[201 / 287] loss= 0.133 lr= 1.00e-04\n",
      "epoch:[26 / 50] batch:[251 / 287] loss= 0.136 lr= 1.00e-04\n",
      "trn_loss: 0.073, val_loss: 0.144\n",
      "training set RMSE 1 cycle: 119, 20 cycle: 94, 100 cycle: 84\n",
      "testing set RMSE 1 cycle: 144, 20 cycle: 143, 100 cycle: 118\n",
      "epoch:[27 / 50] batch:[1 / 287] loss= 0.102 lr= 1.00e-04\n",
      "epoch:[27 / 50] batch:[51 / 287] loss= 0.132 lr= 1.00e-04\n",
      "epoch:[27 / 50] batch:[101 / 287] loss= 0.181 lr= 1.00e-04\n",
      "epoch:[27 / 50] batch:[151 / 287] loss= 0.191 lr= 1.00e-04\n",
      "epoch:[27 / 50] batch:[201 / 287] loss= 0.171 lr= 1.00e-04\n",
      "epoch:[27 / 50] batch:[251 / 287] loss= 0.156 lr= 1.00e-04\n",
      "trn_loss: 0.089, val_loss: 0.168\n",
      "training set RMSE 1 cycle: 138, 20 cycle: 99, 100 cycle: 93\n",
      "testing set RMSE 1 cycle: 145, 20 cycle: 140, 100 cycle: 127\n",
      "epoch:[28 / 50] batch:[1 / 287] loss= 0.188 lr= 1.00e-04\n",
      "epoch:[28 / 50] batch:[51 / 287] loss= 0.188 lr= 1.00e-04\n",
      "epoch:[28 / 50] batch:[101 / 287] loss= 0.129 lr= 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[28 / 50] batch:[151 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[28 / 50] batch:[201 / 287] loss= 0.127 lr= 1.00e-04\n",
      "epoch:[28 / 50] batch:[251 / 287] loss= 0.127 lr= 1.00e-04\n",
      "trn_loss: 0.076, val_loss: 0.146\n",
      "training set RMSE 1 cycle: 126, 20 cycle: 94, 100 cycle: 85\n",
      "testing set RMSE 1 cycle: 138, 20 cycle: 138, 100 cycle: 119\n",
      "epoch:[29 / 50] batch:[1 / 287] loss= 0.162 lr= 1.00e-04\n",
      "epoch:[29 / 50] batch:[51 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[29 / 50] batch:[101 / 287] loss= 0.105 lr= 1.00e-04\n",
      "epoch:[29 / 50] batch:[151 / 287] loss= 0.119 lr= 1.00e-04\n",
      "epoch:[29 / 50] batch:[201 / 287] loss= 0.150 lr= 1.00e-04\n",
      "epoch:[29 / 50] batch:[251 / 287] loss= 0.167 lr= 1.00e-04\n",
      "trn_loss: 0.073, val_loss: 0.132\n",
      "training set RMSE 1 cycle: 125, 20 cycle: 94, 100 cycle: 84\n",
      "testing set RMSE 1 cycle: 136, 20 cycle: 138, 100 cycle: 113\n",
      "epoch:[30 / 50] batch:[1 / 287] loss= 0.181 lr= 1.00e-04\n",
      "epoch:[30 / 50] batch:[51 / 287] loss= 0.112 lr= 1.00e-04\n",
      "epoch:[30 / 50] batch:[101 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[30 / 50] batch:[151 / 287] loss= 0.118 lr= 1.00e-04\n",
      "epoch:[30 / 50] batch:[201 / 287] loss= 0.113 lr= 1.00e-04\n",
      "epoch:[30 / 50] batch:[251 / 287] loss= 0.153 lr= 1.00e-04\n",
      "trn_loss: 0.078, val_loss: 0.144\n",
      "training set RMSE 1 cycle: 131, 20 cycle: 94, 100 cycle: 87\n",
      "testing set RMSE 1 cycle: 148, 20 cycle: 140, 100 cycle: 118\n",
      "epoch:[31 / 50] batch:[1 / 287] loss= 0.198 lr= 1.00e-04\n",
      "epoch:[31 / 50] batch:[51 / 287] loss= 0.225 lr= 1.00e-04\n",
      "epoch:[31 / 50] batch:[101 / 287] loss= 0.155 lr= 1.00e-04\n",
      "epoch:[31 / 50] batch:[151 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[31 / 50] batch:[201 / 287] loss= 0.108 lr= 1.00e-04\n",
      "epoch:[31 / 50] batch:[251 / 287] loss= 0.146 lr= 1.00e-04\n",
      "trn_loss: 0.075, val_loss: 0.114\n",
      "training set RMSE 1 cycle: 130, 20 cycle: 91, 100 cycle: 85\n",
      "testing set RMSE 1 cycle: 131, 20 cycle: 123, 100 cycle: 105\n",
      "epoch:[32 / 50] batch:[1 / 287] loss= 0.125 lr= 1.00e-04\n",
      "epoch:[32 / 50] batch:[51 / 287] loss= 0.152 lr= 1.00e-04\n",
      "epoch:[32 / 50] batch:[101 / 287] loss= 0.178 lr= 1.00e-04\n",
      "epoch:[32 / 50] batch:[151 / 287] loss= 0.099 lr= 1.00e-04\n",
      "epoch:[32 / 50] batch:[201 / 287] loss= 0.173 lr= 1.00e-04\n",
      "epoch:[32 / 50] batch:[251 / 287] loss= 0.122 lr= 1.00e-04\n",
      "trn_loss: 0.058, val_loss: 0.153\n",
      "training set RMSE 1 cycle: 122, 20 cycle: 87, 100 cycle: 74\n",
      "testing set RMSE 1 cycle: 150, 20 cycle: 146, 100 cycle: 121\n",
      "epoch:[33 / 50] batch:[1 / 287] loss= 0.110 lr= 1.00e-04\n",
      "epoch:[33 / 50] batch:[51 / 287] loss= 0.196 lr= 1.00e-04\n",
      "epoch:[33 / 50] batch:[101 / 287] loss= 0.219 lr= 1.00e-04\n",
      "epoch:[33 / 50] batch:[151 / 287] loss= 0.100 lr= 1.00e-04\n",
      "epoch:[33 / 50] batch:[201 / 287] loss= 0.166 lr= 1.00e-04\n",
      "epoch:[33 / 50] batch:[251 / 287] loss= 0.096 lr= 1.00e-04\n",
      "trn_loss: 0.059, val_loss: 0.155\n",
      "training set RMSE 1 cycle: 126, 20 cycle: 84, 100 cycle: 76\n",
      "testing set RMSE 1 cycle: 141, 20 cycle: 145, 100 cycle: 123\n",
      "epoch:[34 / 50] batch:[1 / 287] loss= 0.125 lr= 1.00e-04\n",
      "epoch:[34 / 50] batch:[51 / 287] loss= 0.095 lr= 1.00e-04\n",
      "epoch:[34 / 50] batch:[101 / 287] loss= 0.225 lr= 1.00e-04\n",
      "epoch:[34 / 50] batch:[151 / 287] loss= 0.158 lr= 1.00e-04\n",
      "epoch:[34 / 50] batch:[201 / 287] loss= 0.103 lr= 1.00e-04\n",
      "epoch:[34 / 50] batch:[251 / 287] loss= 0.206 lr= 1.00e-04\n",
      "trn_loss: 0.068, val_loss: 0.158\n",
      "training set RMSE 1 cycle: 131, 20 cycle: 86, 100 cycle: 81\n",
      "testing set RMSE 1 cycle: 139, 20 cycle: 142, 100 cycle: 124\n",
      "epoch:[35 / 50] batch:[1 / 287] loss= 0.116 lr= 1.00e-04\n",
      "epoch:[35 / 50] batch:[51 / 287] loss= 0.155 lr= 1.00e-04\n",
      "epoch:[35 / 50] batch:[101 / 287] loss= 0.228 lr= 1.00e-04\n",
      "epoch:[35 / 50] batch:[151 / 287] loss= 0.092 lr= 1.00e-04\n",
      "epoch:[35 / 50] batch:[201 / 287] loss= 0.152 lr= 1.00e-04\n",
      "epoch:[35 / 50] batch:[251 / 287] loss= 0.110 lr= 1.00e-04\n",
      "trn_loss: 0.052, val_loss: 0.170\n",
      "training set RMSE 1 cycle: 120, 20 cycle: 83, 100 cycle: 71\n",
      "testing set RMSE 1 cycle: 155, 20 cycle: 157, 100 cycle: 128\n",
      "epoch:[36 / 50] batch:[1 / 287] loss= 0.092 lr= 1.00e-04\n",
      "epoch:[36 / 50] batch:[51 / 287] loss= 0.132 lr= 1.00e-04\n",
      "epoch:[36 / 50] batch:[101 / 287] loss= 0.136 lr= 1.00e-04\n",
      "epoch:[36 / 50] batch:[151 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[36 / 50] batch:[201 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[36 / 50] batch:[251 / 287] loss= 0.158 lr= 1.00e-04\n",
      "trn_loss: 0.045, val_loss: 0.153\n",
      "training set RMSE 1 cycle: 124, 20 cycle: 80, 100 cycle: 65\n",
      "testing set RMSE 1 cycle: 154, 20 cycle: 151, 100 cycle: 122\n",
      "epoch:[37 / 50] batch:[1 / 287] loss= 0.137 lr= 1.00e-04\n",
      "epoch:[37 / 50] batch:[51 / 287] loss= 0.183 lr= 1.00e-04\n",
      "epoch:[37 / 50] batch:[101 / 287] loss= 0.156 lr= 1.00e-04\n",
      "epoch:[37 / 50] batch:[151 / 287] loss= 0.108 lr= 1.00e-04\n",
      "epoch:[37 / 50] batch:[201 / 287] loss= 0.085 lr= 1.00e-04\n",
      "epoch:[37 / 50] batch:[251 / 287] loss= 0.139 lr= 1.00e-04\n",
      "trn_loss: 0.056, val_loss: 0.149\n",
      "training set RMSE 1 cycle: 130, 20 cycle: 82, 100 cycle: 74\n",
      "testing set RMSE 1 cycle: 158, 20 cycle: 147, 100 cycle: 120\n",
      "epoch:[38 / 50] batch:[1 / 287] loss= 0.179 lr= 1.00e-04\n",
      "epoch:[38 / 50] batch:[51 / 287] loss= 0.139 lr= 1.00e-04\n",
      "epoch:[38 / 50] batch:[101 / 287] loss= 0.117 lr= 1.00e-04\n",
      "epoch:[38 / 50] batch:[151 / 287] loss= 0.163 lr= 1.00e-04\n",
      "epoch:[38 / 50] batch:[201 / 287] loss= 0.141 lr= 1.00e-04\n",
      "epoch:[38 / 50] batch:[251 / 287] loss= 0.107 lr= 1.00e-04\n",
      "trn_loss: 0.068, val_loss: 0.146\n",
      "training set RMSE 1 cycle: 135, 20 cycle: 85, 100 cycle: 81\n",
      "testing set RMSE 1 cycle: 153, 20 cycle: 141, 100 cycle: 119\n",
      "epoch:[39 / 50] batch:[1 / 287] loss= 0.128 lr= 1.00e-04\n",
      "epoch:[39 / 50] batch:[51 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[39 / 50] batch:[101 / 287] loss= 0.130 lr= 1.00e-04\n",
      "epoch:[39 / 50] batch:[151 / 287] loss= 0.124 lr= 1.00e-04\n",
      "epoch:[39 / 50] batch:[201 / 287] loss= 0.170 lr= 1.00e-04\n",
      "epoch:[39 / 50] batch:[251 / 287] loss= 0.166 lr= 1.00e-04\n",
      "trn_loss: 0.055, val_loss: 0.140\n",
      "training set RMSE 1 cycle: 128, 20 cycle: 83, 100 cycle: 73\n",
      "testing set RMSE 1 cycle: 154, 20 cycle: 145, 100 cycle: 116\n",
      "epoch:[40 / 50] batch:[1 / 287] loss= 0.164 lr= 1.00e-04\n",
      "epoch:[40 / 50] batch:[51 / 287] loss= 0.121 lr= 1.00e-04\n",
      "epoch:[40 / 50] batch:[101 / 287] loss= 0.114 lr= 1.00e-04\n",
      "epoch:[40 / 50] batch:[151 / 287] loss= 0.142 lr= 1.00e-04\n",
      "epoch:[40 / 50] batch:[201 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[40 / 50] batch:[251 / 287] loss= 0.143 lr= 1.00e-04\n",
      "trn_loss: 0.062, val_loss: 0.172\n",
      "training set RMSE 1 cycle: 141, 20 cycle: 83, 100 cycle: 77\n",
      "testing set RMSE 1 cycle: 160, 20 cycle: 148, 100 cycle: 129\n",
      "epoch:[41 / 50] batch:[1 / 287] loss= 0.090 lr= 1.00e-04\n",
      "epoch:[41 / 50] batch:[51 / 287] loss= 0.168 lr= 1.00e-04\n",
      "epoch:[41 / 50] batch:[101 / 287] loss= 0.095 lr= 1.00e-04\n",
      "epoch:[41 / 50] batch:[151 / 287] loss= 0.173 lr= 1.00e-04\n",
      "epoch:[41 / 50] batch:[201 / 287] loss= 0.124 lr= 1.00e-04\n",
      "epoch:[41 / 50] batch:[251 / 287] loss= 0.177 lr= 1.00e-04\n",
      "trn_loss: 0.055, val_loss: 0.161\n",
      "training set RMSE 1 cycle: 129, 20 cycle: 81, 100 cycle: 72\n",
      "testing set RMSE 1 cycle: 157, 20 cycle: 150, 100 cycle: 125\n",
      "epoch:[42 / 50] batch:[1 / 287] loss= 0.128 lr= 1.00e-04\n",
      "epoch:[42 / 50] batch:[51 / 287] loss= 0.150 lr= 1.00e-04\n",
      "epoch:[42 / 50] batch:[101 / 287] loss= 0.090 lr= 1.00e-04\n",
      "epoch:[42 / 50] batch:[151 / 287] loss= 0.149 lr= 1.00e-04\n",
      "epoch:[42 / 50] batch:[201 / 287] loss= 0.120 lr= 1.00e-04\n",
      "epoch:[42 / 50] batch:[251 / 287] loss= 0.114 lr= 1.00e-04\n",
      "trn_loss: 0.046, val_loss: 0.168\n",
      "training set RMSE 1 cycle: 114, 20 cycle: 82, 100 cycle: 67\n",
      "testing set RMSE 1 cycle: 158, 20 cycle: 158, 100 cycle: 128\n",
      "epoch:[43 / 50] batch:[1 / 287] loss= 0.141 lr= 1.00e-04\n",
      "epoch:[43 / 50] batch:[51 / 287] loss= 0.190 lr= 1.00e-04\n",
      "epoch:[43 / 50] batch:[101 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[43 / 50] batch:[151 / 287] loss= 0.126 lr= 1.00e-04\n",
      "epoch:[43 / 50] batch:[201 / 287] loss= 0.126 lr= 1.00e-04\n",
      "epoch:[43 / 50] batch:[251 / 287] loss= 0.160 lr= 1.00e-04\n",
      "trn_loss: 0.049, val_loss: 0.165\n",
      "training set RMSE 1 cycle: 123, 20 cycle: 85, 100 cycle: 68\n",
      "testing set RMSE 1 cycle: 168, 20 cycle: 154, 100 cycle: 126\n",
      "epoch:[44 / 50] batch:[1 / 287] loss= 0.117 lr= 1.00e-04\n",
      "epoch:[44 / 50] batch:[51 / 287] loss= 0.083 lr= 1.00e-04\n",
      "epoch:[44 / 50] batch:[101 / 287] loss= 0.111 lr= 1.00e-04\n",
      "epoch:[44 / 50] batch:[151 / 287] loss= 0.130 lr= 1.00e-04\n",
      "epoch:[44 / 50] batch:[201 / 287] loss= 0.145 lr= 1.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:[44 / 50] batch:[251 / 287] loss= 0.133 lr= 1.00e-04\n",
      "trn_loss: 0.053, val_loss: 0.154\n",
      "training set RMSE 1 cycle: 130, 20 cycle: 79, 100 cycle: 72\n",
      "testing set RMSE 1 cycle: 159, 20 cycle: 145, 100 cycle: 122\n",
      "epoch:[45 / 50] batch:[1 / 287] loss= 0.130 lr= 1.00e-04\n",
      "epoch:[45 / 50] batch:[51 / 287] loss= 0.153 lr= 1.00e-04\n",
      "epoch:[45 / 50] batch:[101 / 287] loss= 0.178 lr= 1.00e-04\n",
      "epoch:[45 / 50] batch:[151 / 287] loss= 0.122 lr= 1.00e-04\n",
      "epoch:[45 / 50] batch:[201 / 287] loss= 0.125 lr= 1.00e-04\n",
      "epoch:[45 / 50] batch:[251 / 287] loss= 0.130 lr= 1.00e-04\n",
      "trn_loss: 0.046, val_loss: 0.174\n",
      "training set RMSE 1 cycle: 123, 20 cycle: 77, 100 cycle: 66\n",
      "testing set RMSE 1 cycle: 166, 20 cycle: 161, 100 cycle: 130\n",
      "epoch:[46 / 50] batch:[1 / 287] loss= 0.108 lr= 1.00e-04\n",
      "epoch:[46 / 50] batch:[51 / 287] loss= 0.114 lr= 1.00e-04\n",
      "epoch:[46 / 50] batch:[101 / 287] loss= 0.148 lr= 1.00e-04\n",
      "epoch:[46 / 50] batch:[151 / 287] loss= 0.092 lr= 1.00e-04\n",
      "epoch:[46 / 50] batch:[201 / 287] loss= 0.116 lr= 1.00e-04\n",
      "epoch:[46 / 50] batch:[251 / 287] loss= 0.174 lr= 1.00e-04\n",
      "trn_loss: 0.047, val_loss: 0.162\n",
      "training set RMSE 1 cycle: 124, 20 cycle: 80, 100 cycle: 67\n",
      "testing set RMSE 1 cycle: 150, 20 cycle: 153, 100 cycle: 125\n",
      "epoch:[47 / 50] batch:[1 / 287] loss= 0.083 lr= 1.00e-04\n",
      "epoch:[47 / 50] batch:[51 / 287] loss= 0.125 lr= 1.00e-04\n",
      "epoch:[47 / 50] batch:[101 / 287] loss= 0.111 lr= 1.00e-04\n",
      "epoch:[47 / 50] batch:[151 / 287] loss= 0.121 lr= 1.00e-04\n",
      "epoch:[47 / 50] batch:[201 / 287] loss= 0.140 lr= 1.00e-04\n",
      "epoch:[47 / 50] batch:[251 / 287] loss= 0.116 lr= 1.00e-04\n",
      "trn_loss: 0.041, val_loss: 0.186\n",
      "training set RMSE 1 cycle: 118, 20 cycle: 75, 100 cycle: 62\n",
      "testing set RMSE 1 cycle: 166, 20 cycle: 166, 100 cycle: 134\n",
      "epoch:[48 / 50] batch:[1 / 287] loss= 0.155 lr= 1.00e-04\n",
      "epoch:[48 / 50] batch:[51 / 287] loss= 0.186 lr= 1.00e-04\n",
      "epoch:[48 / 50] batch:[101 / 287] loss= 0.162 lr= 1.00e-04\n",
      "epoch:[48 / 50] batch:[151 / 287] loss= 0.103 lr= 1.00e-04\n",
      "epoch:[48 / 50] batch:[201 / 287] loss= 0.154 lr= 1.00e-04\n",
      "epoch:[48 / 50] batch:[251 / 287] loss= 0.152 lr= 1.00e-04\n",
      "trn_loss: 0.052, val_loss: 0.179\n",
      "training set RMSE 1 cycle: 130, 20 cycle: 73, 100 cycle: 70\n",
      "testing set RMSE 1 cycle: 170, 20 cycle: 160, 100 cycle: 131\n",
      "epoch:[49 / 50] batch:[1 / 287] loss= 0.141 lr= 1.00e-04\n",
      "epoch:[49 / 50] batch:[51 / 287] loss= 0.150 lr= 1.00e-04\n",
      "epoch:[49 / 50] batch:[101 / 287] loss= 0.113 lr= 1.00e-04\n",
      "epoch:[49 / 50] batch:[151 / 287] loss= 0.135 lr= 1.00e-04\n",
      "epoch:[49 / 50] batch:[201 / 287] loss= 0.110 lr= 1.00e-04\n",
      "epoch:[49 / 50] batch:[251 / 287] loss= 0.137 lr= 1.00e-04\n",
      "trn_loss: 0.046, val_loss: 0.150\n",
      "training set RMSE 1 cycle: 126, 20 cycle: 77, 100 cycle: 66\n",
      "testing set RMSE 1 cycle: 157, 20 cycle: 152, 100 cycle: 120\n",
      "epoch:[50 / 50] batch:[1 / 287] loss= 0.151 lr= 1.00e-04\n",
      "epoch:[50 / 50] batch:[51 / 287] loss= 0.119 lr= 1.00e-04\n",
      "epoch:[50 / 50] batch:[101 / 287] loss= 0.125 lr= 1.00e-04\n",
      "epoch:[50 / 50] batch:[151 / 287] loss= 0.090 lr= 1.00e-04\n",
      "epoch:[50 / 50] batch:[201 / 287] loss= 0.094 lr= 1.00e-04\n",
      "epoch:[50 / 50] batch:[251 / 287] loss= 0.172 lr= 1.00e-04\n",
      "trn_loss: 0.053, val_loss: 0.156\n",
      "training set RMSE 1 cycle: 136, 20 cycle: 83, 100 cycle: 72\n",
      "testing set RMSE 1 cycle: 166, 20 cycle: 153, 100 cycle: 123\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEOCAYAAABxdpuaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAIABJREFUeJzt3XmczPUfB/DXe9eyWPcVK3YlV4hsJESSK1eOUioqSVTSRacoxS+hA5EOKckRqZwJq4RWlkW5LUvuM6y1O+/fH++ZZnZ3Zuf7nXt338/HYx4z853vfL/v7xrzns9NzAyllFLKjLBgB6CUUir30eShlFLKNE0eSimlTNPkoZRSyjRNHkoppUzT5KGUUso0TR5KKaVM0+ShlFLKNE0eSimlTCsQ7AD8pWzZshwTExPsMJRSKlfZtGnTSWYu526/PJs8YmJikJCQEOwwlFIqVyGiZCP7abWVUkop0zR5KKWUMk2Th1JKKdPybJuHUip/unr1KlJSUpCamhrsUEJaZGQkKleujIiICI/er8lDKZWnpKSkoFixYoiJiQERBTuckMTMOHXqFFJSUhAbG+vRMbTaSimVp6SmpqJMmTKaOHJARChTpoxXpTNNHkqpPEcTh3ve/o00eSillDJNk0dWqalA69bAjBnBjkQppUKWJo+sChUC1q8HkpKCHYlSKhc6e/YsJk+e7JNj9evXD/PmzfPJsXxNk0dWREClSsCRI8GORCmVC7lKHhkZGUGIxn+0q64zmjyUyjNatcq+7Z57gEGDgEuXgI4ds7/er5/cTp4EevbM/Nrq1Tmfb/jw4di7dy8aNGiAiIgIREVFoWLFikhMTMTixYvRoUMHNG/eHOvWrUN0dDS+//57FC5c2O11rFy5Es8//zzS09Nx8803Y8qUKShUqBCGDx+ORYsWoUCBAmjbti3GjRuHuXPnYuTIkQgPD0eJEiUQHx/v9vhmacnDmeho4PDhYEehlMqFxowZg+uuuw6JiYl49913sXHjRowePRo7duwAAOzevRuDBw/G9u3bUbJkScyfP9/tMVNTU9GvXz98++23SEpKQnp6OqZMmYLTp09jwYIF2L59O7Zu3YpXX30VADBq1CgsW7YMW7ZswaJFi/xynVrycOamm4CLF4MdhVLKB3IqKRQpkvPrZcu6L2m407hx40wD8WJjY9GgQQMAQKNGjXDgwAG3x9i5cydiY2NRo0YNAEDfvn0xadIkPPnkk4iMjET//v1x1113oVOnTgCAZs2aoV+/frjnnnvQvXt37y7ABS15OPPCC4CfsrVSKn8pWrRopueFChX673F4eDjS09PdHoOZnW4vUKAANm7ciB49emDhwoVo3749AODjjz/GW2+9hUOHDqFBgwY4deqUF1fgnJY8lFLKh4oVK4YLFy749Ji1atXCgQMHsGfPHlSvXh0zZ85Ey5Yt8e+//+LSpUvo2LEjbrnlFlSvXh0AsHfvXjRp0gRNmjTBDz/8gEOHDqFMmTI+jUmThzNbtgC9egHTpjlvbVNKKRfKlCmDZs2aoW7duihcuDAqVKjg9TEjIyPx+eefo1evXv81mA8cOBCnT59G165dkZqaCmbGhAkTAAAvvPACdu/eDWbGHXfcgRtvvNHrGLIiV8Wh3C4uLo49Xklwzx7g+utloOBDD/k2MKWUX/3111+oXbt2sMPIFZz9rYhoEzPHuXuvtnk4U7Gi3Gt3XaWUckqrrZwpWhQoUUKTh1IqIAYPHozffvst07YhQ4bg4YcfDlJE7mnycEUHCiqlAmTSpEnBDsE0TR6udO4MFCsW7CiUUiokafJwZezYYEeglFIhK6AN5kTUnoh2EtEeIhru5PVniWgHEW0lopVEVNXhtQwiSrTeAjOCj1luSimlMglY8iCicACTAHQAUAfAfURUJ8tumwHEMXN9APMA/M/htcvM3MB66+L3gD//XOYu8MPITKWUyu0CWfJoDGAPM+9j5jQAswF0ddyBmVcx8yXr0/UAKgcwvsyKFZOFobTRXCnlR1FRUS5fO3DgAOrWrRvAaIwLZPKIBnDI4XmKdZsrjwJY4vA8kogSiGg9EXXzR4CZVKok95o8lFIqm0A2mDtbbd1pgwIRPQAgDkBLh81VmPkIEVUD8AsRJTHz3izvGwBgAABUqVLFu2g1eSiVNwR4QY9hw4ahatWqGDRoEADgjTfeABEhPj4eZ86cwdWrV/HWW2+ha9euOR4nq9TUVDzxxBNISEhAgQIFMH78eNx+++3Yvn07Hn74YaSlpcFisWD+/PmoVKkS7rnnHqSkpCAjIwOvvfYa7r33XlPncyeQySMFwLUOzysDyPbNTERtALwCoCUzX7FtZ+Yj1vt9RLQaQEMAmZIHM08DMA2Q6Um8ilZHmSulPNC7d28888wz/yWPOXPmYOnSpRg6dCiKFy+OkydP4pZbbkGXLl1A5Ow3tXO2sSBJSUn4+++/0bZtW+zatQsff/wxhgwZgj59+iAtLQ0ZGRlYvHgxKlWqhJ9++gkAcO7cOZ9fZyCTxx8ArieiWACHAfQGcL/jDkTUEMBUAO2Z+bjD9lIALjHzFSIqC6AZMjem+16hQsDgwUD9+n49jVLKzwK8oEfDhg1x/PhxHDlyBCdOnECpUqVQsWJFDB06FPHx8QgLC8Phw4dx7NgxXHPNNYaP++uvv+Kpp54CILPsVq1aFbt27ULTpk0xevRopKSkoHv37rj++utRr149PP/88xg2bBg6deqEFi1amLoGIwLW5sHM6QCeBLAMwF8A5jDzdiIaRUS23lPvAogCMDdLl9zaABKIaAuAVQDGMPMOvwf90UdAF/937FJK5S09e/bEvHnz8O2336J37974+uuvceLECWzatAmJiYmoUKECUlNTTR3T1SS2999/PxYtWoTChQujXbt2+OWXX1CjRg1s2rQJ9erVw0svvYRRo0b54rIyCeggQWZeDGBxlm2vOzxu4+J96wDU8290Tk8MXLgAFC8e8FMrpXKv3r1747HHHsPJkyexZs0azJkzB+XLl0dERARWrVqF5ORk08e87bbb8PXXX6N169bYtWsXDh48iJo1a2Lfvn2oVq0ann76aezbtw9bt25FrVq1ULp0aTzwwAOIiorCF1984fNr1BHmOXn8ceCnn3Q9c6WUKTfccAMuXLiA6OhoVKxYEX369EHnzp0RFxeHBg0aoFatWqaPOWjQIAwcOBD16tVDgQIF8MUXX6BQoUL49ttv8dVXXyEiIgLXXHMNXn/9dfzxxx944YUXEBYWhoiICEyZMsXn16jreeTktdeAt98G0tKA8HDfBKaU8itdz8M4Xc/DXypVAiwW4NixYEeilFIhRautchJtHcN45Ih93IdSSvlYUlISHnzwwUzbChUqhA0bNgQpIvc0eeREBwoqlSsxs6kxFMFWr149JCYmBvSc3jZZaLVVTqpVA15/HahePdiRKKUMioyMxKlTp7z+cszLmBmnTp1CZGSkx8fQkkdOSpcGRo4MdhRKKRMqV66MlJQUnDhxItihhLTIyEhUruz53LOaPNw5eVJm1/Xij6yUCpyIiAjExsYGO4w8T5OHO+3bA+XLA4sXu99XKaXyCW3zcKdSJW0wV0qpLDR5uKPJQymlstHk4U6lSsCJEzLKXCmlFABNHu7ZxnocPRrcOJRSKoRo8nDnttuAadN0Zl2llHKgva3cqVFDbkoppf6jJQ93LBbgzz+BAweCHYlSSoUMTR5G3HILMHVqsKNQSqmQocnDnbAwoGJF7a6rlFIONHkYoWM9lFIqE00eRmjyUEqpTDR5GKHJQymlMtGuukb07w907AgwA7logRmllPIXTR5G3Hij3JRSSgHQaitjzp0DFi3SqiullLLS5GFESgrQtSuwdm2wI1FKqZCgycOI6Gi5P3w4uHEopVSI0ORhRIkSQOHCWm2llFJWmjyMINLuukop5UCTh1GaPJRS6j/aVdeojz4CChUKdhRKKRUSNHkYVb9+sCNQSqmQodVWRu3eDUyeDFy8GOxIlFIq6DR5GJWQAAweDBw8GOxIlFIq6DR5GFWpktxro7lSSgU2eRBReyLaSUR7iGi4k9efJaIdRLSViFYSUVWH1/oS0W7rrW8g4wagyUMppRwELHkQUTiASQA6AKgD4D4iqpNlt80A4pi5PoB5AP5nfW9pACMANAHQGMAIIioVqNgByGqCgCYPpZRCYEsejQHsYeZ9zJwGYDaAro47MPMqZr5kfboeQGXr43YAVjDzaWY+A2AFgPYBiltERQHFi2vyUEopBLarbjSAQw7PUyAlCVceBbAkh/dGZ30DEQ0AMAAAqlSp4k2szm3aBJQv7/vjKqVULhPIkoezVZTY6Y5EDwCIA/Cumfcy8zRmjmPmuHLlynkcqEvVq0vpQyml8rlAJo8UANc6PK8MIFsdEBG1AfAKgC7MfMXMe/0uJQV49llg+/aAn1oppUJJIJPHHwCuJ6JYIioIoDeARY47EFFDAFMhieO4w0vLALQlolLWhvK21m2BRQRMmACsWBHwUyulVCgJWPJg5nQAT0K+9P8CMIeZtxPRKCLqYt3tXQBRAOYSUSIRLbK+9zSANyEJ6A8Ao6zbAis6GoiNBX79NeCnVkqpUELMTpsdcr24uDhOSEjw/YEfeghYtgw4elRKIkoplYcQ0SZmjnO3n44wN6t5c+D4cWDPnmBHopRSQaPJw6zmzaW7rs5xpZTKx3RKdrNq19YqK6VUvqfJwyxNGkoppdVWHlm8WAYMHjsW7EiUUiooNHl4olQpYO9e4Lffgh2JUiqvOH8+2BGYosnDEzfdBERG6ngPpZRvJCXJzN3TpgU7EsM0eXiiUCGgcWNNHkop36hTB7jtNuDxx4FPPw12NIZo8vBUixbAn3/qmuZKKe9YLEBYGLBgAdC+PdC/PzB9erCjckuTh6fatwf69QMuXAh2JEqp3GzpUqBqVWD/fnsCeewx4Ntvgx1ZjrSrrqeaN5ebUkp54/vvgTNngGrVpEp8wQLgxReBVq2CHVmOtOThDYtFR5orpTxnsQA//CCljUKFZFtkJPDBB0CFCsDVq8Dy5cGN0QVNHt548UVp6EpPD3YkSqncKCEB+OcfoGtX56+PHw+0awesXh3QsIzQ5OGNm2+WBvPExGBHopTKjb7/HggPBzp2dP76008DhQsDCxcGNi4DtM3DG82ayf2vvwJxbmcwVkqpzO68U5a2Ll3a+euFC8v3zKpVgY3LAC15eKNyZSAmRsd7KKU806oVMGyY+322bgVOnQpERIZp8vBW8+aSPPLoolpKKT9Zvx7YssX9frffLvfr1vk3HpO02spbgwcDvXpJr4nw8GBHo5QKNmbg0iVg6FCgQwfg7rud7zd8OHD6tJQqcnLzzcC+fVLLEUI0eXjrllsyP09Pl18IJUoAN94oH6TUVKm7VErlfWPHAkuWyIqjixcDbdoAxYpl3ufUKWDtWuDll90fLyICiI31T6xe0GorX9iwQdY2v/9+oFw5oGVLYMIEKY20bw888USwI1QqOJYuBf7+O/DnPXFCur9+9x3w00+BOy8z8Pnn8viLL4AjR4DXX8++3+LF8v3QpYux4yYmAvfdJ9cVKpg5T94aNWrEAXP33cwAc7lyzP36Mc+bx3zunLw2bJi8tmFD4OJRKhRs2SKf/TJlAn/u77+Xc1esyFy0KHNysrH3paZ6d9716+W806fL84EDmcPCmDdtyrxfjx7MlSoxZ2QYO+7vv8tx5871Lj4DACSwge9YLXn4wtSpMtjn6FH51dGjh3S/A4BXXgGuuUb6a1sswY1T5R4//wyMHh3sKDxnsdhL3A88EPiBtOvWAQUKAL/8IqWBQYNy7tTCDLz0EhAVBfTpA2ze7Nl5Z8yQKupeveT5O+8AZcsCzz1n3ycjA1izBujcWSZENKJRI4ktlLrsGskwthuAcgDKOTyvB+AtAPeZOU4gbgEtebjz+efyq2HmzGBHonKLNm3kM3PoULAj8cz06RL/558H5/y33cbcuLE8Hj9eYpk92/X+b78t+7RowVysGPOrr8r2jAzm9HRj50xNZS5Vivn++zNvX706e8nnwgXmo0eNHdemfXvm2rXNvccDMFjyMJs8VgF4xPq4LIAzALYDOAfgOTPH8vctpJJHRgZzXJx8mC2WYEejcoOdO+W/5/vvBzsSz/Tuzdy8uXz2r15lnjOH+eDBwJw7LY05MpL5mWfkeXq6/P8rX5751Kns+2/dykzE/MADEu+5c8xnzshrCxYwV6vG/MEHch05uXiReeJE5nXrnL9uscg+nho7Vj4TZpOOSUaTh9lqq/oA1lsf9wSwh5lvAPAQgMc9L//kcWFhwJw5UoQmCnY0KtSdPAlcdx1Qrx4wb16wo/HMrFnAjz/KZ//IEWns/eCDwJz7+HGgQQNZXAmQLvTTpwN33eX8/1+9ejL54GefSbzFiwMlS8prpUrZq53feCPn8xYpAgwZAjRtmv01ZmnA79cP6NRJZs41q3VroG5d4PBh8+/1ByMZxnYDcAlAFevjeQBesz6+FsBlM8fy9y2kSh6OUlOZT5wIdhQqlHXvztygAfPIkfKL+MiRYEdk3Natzhun77mHuUQJqa4JFd9/z/zLL8b27ddPGr7Xr3f++rFjzJ99lvP1jRwpJQeA+csvzccbIPBTyWM3gO5EdC2AtgBscwVXAHDW+1SWx2VkAE2aAAMHBjsSFapSU4Fly2T8UM+eQNu2stZDbpCeLo3j7dpl7xwydChw7px0X/W3nDqmbN0K3HuvdN/t1QsYOdLY7BATJwLR0dIl/+rV7K9//TXwyCM5L9EwbBhQs6aUhO66y/05XcnI8Py9vmQkw9huALoDuAIgA8Byh+2vAFhs5lj+voVsyePNN+WXh9FfPCp/WbxYPh9LlgQ7EvNsDdPffef89SZNmKtXN9491VPXX8/8+uvOX1uxwv7rv25d520grqxaxbxokfPXGjSQdhV3tm+X9h9PzZvHXLy4X0uj8EeDuRwXFQA0BBDmsK0JgFpmj+XPW8gmj0uXmKtWZa5f330DnMp/Bg5kjorKPN7gyBH7uKFQdeiQxN2xo+tOIbNnM9eoYXzMhScOHmS3HQ0GD5bE4c0XsOO/j208y4cfen48oxIS5FyzZvntFEaTh+lxHsx8jJk3M7MFAIioOoAtzByEYaS5UOHCwP/+J8XnH38MdjQqlDADixZJtY9tVbndu6W6JMTXs8Zzz0m11Ycfuu4U0rMn8NdfQJUq/ovj99/l/tZbXe/z0Ufy/69iRc/O8ckn0sh+/rw8nzFDphDp3duz45nRoIFMfRQC4z1MJQ8iepuI+lofExGtALALwD9E1MQfAeZJd98tvTjMfAD69XM+zYHKO5iBL78EXnjBvq16del5FWq9rphlYKytfeGaa4DXXpN1uF0JD5feTJcuSY8of1i3Tn6g3Xhjzvt50+uxbl1g717g2Wfl+V9/SRtG2bKeH9Oo8HDpRRYCycNslVUygFusjzsCOAGgMYD3Aawycyx/30K22somOdn4mI/Dh+31tCr/GT6cOTyc+eTJYEcin9vRo5lr1pTP46pVsv3cOWOf56tXmatUkd5L/tC4sQwQ9Lfhw+X6f/xRnnszfsMsW9uSqwGkmzd7dXj4qdqqAoAU6+OOAOYw80YAH0LaQZRRVaoY//Uzf77c79jhv3hU8E2Y4Hx67p49pYfNokWBj8nmyBHp+RUTI1PuVKggYyduukleL17c2Oe5QAGZlmPWLJnOx9d69QIefdT3x83qjTek6uqhh2SG3CJF/H9Om44d5d8g6xIQGRnA888DDRvKmDJ/M5JhbDcAhwE0sz7eBaCH9XEtAOfMHMvft5AveTAzP/WU9P12p0ULaeBjZr582b8xqeA4cEB+Tb77bvbXLBbmmBhpjA6kxETp/cUso7YbN2YeMYJ5717vjrtrl1zryy97HWJQbd4s1zF1arAjEV9+KfE8+aRXnXHgp+lJPoBUXa0AcBJAUev23gA2GXh/ewA7AewBMNzJ67cB+BNAOoCeWV7LAJBovS1yd65ckTw6dZKeVzkV90+elCqLUaOYBw1irlcvYOGpAPrwQ/nvuHOn89fXrmVOSQlMLNu2MTdqJPFUr+6fKXW6d5fj9+7tu+Pv3Wuu660v7N4d2Corm4sXmdeskce2v19GBvNPP3l9aKPJw2y11bPWBLIDwJ3MfNG6vSKAKTm9kYjCAUwC0AFAHQD3EVGdLLsdBNAPwCwnh7jMzA2sN4OT4Ie4u+8GkpNzXoqyTBkZeDRwoDSeJiUB+/cHLsZA+fJLYNOmYEcRPIsWAbVqATVqOH+9eXPpdRUIEybIGhzvvy9LpfpjSp2vvgJGjQKqVrUf/8oV74757LPOpwbxp+rVA1tlZfPJJ7Ju0Pz5UnWYnCydETp2DFgIppIHM6cz83vMPISZNztsn8DM0928vTFkLqx9zJwGYDaArlmOf4CZtwLIH3OX26ZkXrgw5/0qVZJFpjp3lud5rYvv2bNA375A48bBjiQ4zp0DVq92vzDQd98FZpr2xET5En76afnx4g+FC0vvrDFj5Hl8vPTU+vJLz5YuYJaeVjl10c1LbOua9+wp/38uXQp4CKbHeRBRBSIaRUTziGguEY0kovIG3hoN4JDD8xTrNqMiiSiBiNYTUTcXsQ2w7pNwIpRW3HKlXDmgWTPXyePwYVkDOTFRnlevLr9Of/ghcDEGwooVcv/ee94f69gxacTMLVN6AMD27UBkpPvkER8PvPkmcOGC/2JhBi5elPEEgRQVJSWrvn1lze7PP5c4jNq7V1bZyy/Jo25d6cLdrBmwcSNQu3bAQzA7zqMZpL3ifgCXAaQC6ANgDxG5Ky86K/samFTmP1WYOc567olEdF22gzFPY+Y4Zo4rV66ciUMHUb9+8oF3tljOvHmyjKfj+uedO8uvVNsApbxg8WIZ9/Lkk94f6+23gZkz7UuB5ga33ioz6bqrcunRQ6p2/LmsKpGMWxg71n/ncOamm6SKbMYMSRqPPCJJhA1+RaxbJ/eBrrYKlrAwYNs2WQc9SN91Zkse4wB8A6AGMz/IzA8CqAGpgnL3szEFMvuuTWUAR4yemJmPWO/3AViNvNI1+JFHgMmTpQtjVnPmyGCnmjXt2x58UOqi88rU7hYLsGQJ0KqVlKi2b/fueLbuzLbSWqjYt0+6dGZl+3IsWND9qnK33iqD8QIxYNDoCne+PudDD0nyWrsWeOst+Zynp8uo+0mTnE9KCEjyKF4cqJO1GTUPi4wM7veAkVZ12w1S2qjpZHstuJmSHUABAPsAxAIoCGALgBtc7PsFHHpbASgFoJD1cVnI7L51cjpfruhtZZORIROmObLN0TN6dHBiCpT0dOYffpCJIsPDve+++fffsjZ05cqhs/DWyZOyONE998hzi0V66TDLddepk/3f35VBg5gLF2Y+f94/sY4dKz2gQklysr331113Oe/dtHOnfcCe8gr81NvqnPXLP6tYuJmSnZnTATwJYBmAvyADDLdb20+6AAAR3UxEKQB6AZhKRLafobUBJBDRFshqhmOYOe+MmBszRgYcOf4ytf26tK2F7OjECZnaOlSmZvZGeLgsjnP77fI3+OMP745Xs6b8av3zT+9/lX35pZT0jhguIDs3f75Mtd7V2j8kMRG4/nqpp37hBamvNzrfU79+MlDv0CF3e3pmxQpgzx7/HNtTVarIVCgffyyl1DZtgNOnM+9To4Z305wr84xkGLaXACZCBgr2gSSMGAAPQKqkxps5lr9vuarkYZsp84sv7Nu++oq5Tx/n+8+eLfv/+qvn57xyJTR+mU+bxpyUJI8HDGAuWdLzKbtnzGCeO9d3sTVuzD4ZBNaqlUznYft7Hz8uy5q2acNcoABzz57mjmfm77NxI3PfvsbGP1gszGXLMj/6qLl4Amn+fOaCBZnvvde+bds2+bf/99/gxZWHwE+DBAtC5rGyrelhgTSaTwBQ0Myx/H3LVcnDYpFqlm7djO1/5ox86Qwfbv5ctvlwhg5lbt2aef9+88dwx+j04SdOyEp5I0bI8+nTOceBcjmxWGSt6U6d5PlnnzG//bb549ikpsqX1IMP2r/04+Pta1sbdfhw5mvM6sIFSeRmnT/vem0Jm6tXmRs2lGo8I/8mKSkcsKnFvREfn3k69TfekL9xqE9bn0sYTR5mx3mkMfMQSBtEA+utNDMPZRm7oTxBBHTrJivIXbok03Dn1E2xZEmgRQvzXXb37JG+9J9+Kg2LGzdKVdHHHxvv1eLOunVSzbB6NfDPPznvu2yZnNdW3WAb55GQYP68e/ZIo3T79vI8Ph4YN86zMQOADNxMS5Pus0Ty79GtG1C/vrl5g9avl4ZgV9N1R0VJY7lZo0bJINOdO13vM3kysHmzzPX0zDPu55KyDVYNdDdds1q0kOnU09OlGu/bb+VzXLx4sCPLX9xlFwCLjN6MZKtA3XJVyYOZ+eef5VffggWy4tqtt+a8v21mzX37jJ+jTx9pbP3nH3l+4IBUnQBSCjlwwPP4maVqpEoV5thY5jFjmCMiXM/8ycx8//3M5crZq2HS06XU4Um11fvvy3XY5l2aOVOe//mn+WMxyzxBRNJxwWbjRlnMCGB+/nnj1X7+mDLj6FFZfKl7d+evHz7MXKwYc7t2zBs2SMzuVrBbsoS5adPc8wv+yBGZPgVgfvzxYEeTZ8BX1VYAPjd6M3LCQN1yXfJIS5N5aXbulH+WMWNy3n/3btnvyy+NHT8pSb4Mhw3LvN1ikTr9cuWkp5KnLBbmrl0lYWzcKEktLCz7+WzS05lLl2Z+6CHPz+moQwdZftTGVgUzbpznx3Q2zfjFi8z9+3Om6bhd8Xeb0siREse6ddlf69uXuVAh+ZykpTEXKSITceY1R4/KZ8jTHwkqG58lj9x6y3XJw2bcuMy/oHNy+LDx43brJmsfu/oVfOmS/XGvXvLL2szxbb/8J0ywb+venblUKecNmTt2SPfV2bMzb1+3Tr6czbQDZGTIGtJPP515e82a/pmJNi1NEpW7GZHHjmW+/fbMS5b60oULzBUqMDdvnj1RpaRI47LNHXfI3ygn/l5bXOUKRpNHEEYCKZdOn5b5+IGcV2SzqVTJ2HGPHQPKF+maAAAgAElEQVRWrpRjly7tfB/bKParV2XA4vjxQGws8NhjwK5d7s9x4IC0DwwZYt82dKhMEzJjRvb9a9eW6+3aNfP2gwdlnYht2wxdGgBpU9i8GXj33czb27WTNg822Z5z+rS8d+1a569HREj7gLuVHb/+Wrro2paU9bWoKGDkSPm3s804kJ4u1xsdDXTvbt+3eXOJ+dw558e6cEHaDD75xD+xqrzHSIbJjbdcWfK4cEF+vY8da2z/48elRPH998b2vXDBeCx79zI/8YSUDoiMDcBKS8v83GJhjotz/4vX0b598jeYMsX4e1zxtNpo6VKJYeVK9/smJTn/xb59uxzj/fc9i8GorNf4xhvMLVtmLkkyS5va9dfbu0Vn9dtvEq+7Hlwqz4OWPHKhqCj55e+4hnVOSpUC1qyxrzTozOnT8ku0XDk5vlHVqklvnQMHgJdflnmGAOmp1bYtMHWqlGief97eOyoiIvMxiGQw48qVmbcfPSrHi4/Pft6YGFkL2sxgwVtvtc/OmvX8gPmSx4YN8t64uJz3W7NGevksWJD9tdmzpUR0zz3mzm2W7RqTk6XU8Pbb0hPJcT40AGjdWkqQdes6P46tp5W7tb+VstLkEWoKFDA+MrpAAan2mTlTuiwmJ2d+nVmqkhyrL8yqUEHmGCpvnTg5LEzWExk4UL6k3nsP+Pln1++/4YbsVWVLl0rCKVEi+/5E0mV340Zj8e3fD/z+e/YvS5sBA2RmYjM2bJCuzO66fjZvLrMcjxiRuUswsySPVq1kLqpAeOQRudbISKlyzMpdIk1MlB8j117r/HWlstDkkdtNnAg895x8WdWokXkN7KVLgd9+k5KCrzz8sPyC3bIFePVV4MUX5ZaT7dslIdhiW7xY2mvq13e+f5MmUnfvahI8R0uXyr1tfEdWxYvLmJPLl90fC5Av140bJQZ3wsMlcWzfDsyda9+eng4MGiRjKwLlzTdlvMjYsZLUnfn8c0kOqanZX0tMlPEdeWXCTeV/Ruq2cuMtV7Z5eOPgQeY337TXga9eLaOLY2I8G8HsS6dOSVfRhx+WdpESJaRHlStm2iq6dJFrdPWen36SuvyffzZ2vLNnpYfUjBnG9k9PZ77hBuZateRxMLkbn7Fwofwt4uOzv/buu8yff+6XsFTuAu2q613yCPb3gFfOn5cBYoDxL0F/e+IJme5jzhyJ67vvvD9maipz0aJybFfOn5epXLydrTcnc+dKHImJ0ng+a5YkoVBz8iTni5malVeMJg+ttnJi1ixpg/XXxKV+V6yYNFKPHw/06RPsaMSQITLdx6efSjvNHXfkvP/AgcATT+S8z6VLsk9OjdLFikmVmdEpRTyZzqR7d+lYcOONMj3L/ff7d8EmT5UpI205WbsgHz/ufK0RpXLgZAUiVaSIrCkUFydV2bfdFuyIPHDzzfYeUqGgZk2Zw2rjRhnLERmZ8/5nzkjDdU5Klco+tsOZgQNlGnsjWrWStqPp043tD0gngrJlpb1k/HhpvHe3pGywtGgBfPONTOcfHi7bxo+X28WL2XvMKeWCljyc6NZNvuNKlpQfyB984Lt5A/O1V14BRo821ijbuLH0Hjt+3PU+GzYYa1R/8EHg2Wfd75eWZv+H90S/ftJtt107c92iA6l7dxn4eemSfVtiovSK08ShTNDk4ULt2vI90qGD1Lj89luwI8oDmjaVLy4jI65tpSZX4z0OHQJuuQX48ENj5z59GkhKynmfrVtljXDb7L5m2UobDzzg2fsDoW1bmW24WDH7NltPK6VM0OSRgxIlgIULZfGy5s1lm5ZAAuSmm6Q6yNV4j6++kvt27Ywd79573X+p26rJjHTTdaZHD1kVsEcPz94fKFeu2KecOXpUBnvq4EBlkiYPN8LC7EMIVq6UKmPbNELKj6Ki5Mve2aC1Dz6QUe/t20sDsBGtW0vJwl01WIUKxpeEdcbInGTBNmAA0LKl/BLKLWt4qJCjycOEsDBZ26dfPy2BBMSMGUD//pm3ffyx1CPefbcUC40OamvdWu6XLXO9T5s2MuAyrw+Uu/VWKXHs3SvTq0yfLiU9pUwgzqPfgnFxcZzgyYp0bowfL98v77wDDB/u88OrrNLSJFPb2kkOH5Z2jrfekulZjEpPlxHt589L20epUv6JNzfYsUMayD/7TGYMUMoBEW1iZjcTu2nJw7ShQ6X6/JVXgOXLgx1NHrdzpzTszpsnEzFmZMhU42PGmEscgOw/c6Z02XVW+jh+HDhyxDdxh7ratWXMx9q1Unrbvz/YEalcSJOHSUT2JcAXLgx2NHlctWpSVzhokIzV8HbgXaNGss65s/XEp02TxORqvYu8hEh6gCxbJo37X3wR7IhULqTJwwNFi8ps3JMmBTuSPC4iAmjYUKqaJkzwzcC76Gi5X706c0ljwwaZIdfZTL950UsvSTHaYtHGcuURTR4eKl1afsDt3g288YY2oPvN5MnAihW+naH21CmgUydpjJfZniR5eNpFNzdq0sQ+5bwmD+UBnZ7ES3Pnykqghw5J79JTp6TTTr9+UgPSsqUMMCxaNNiR5lL++GIrU0baTZ56Sqqr2rWTthBPBwfmVuPGyX1MTFDDULmTJg8vDR8uwwc++0x+yNnmngOAf/6RbvRz5minlpAzaBDw/fcybckrr8i2/FTyAKTKqlq1vN81WfmFdtX1katXs08NxCyJpFQpmWzVG089JccZNcq74ygHKSmyLGuZMsBrr8kMxPlpfqcrV+TeyHQxKt8w2lVXk4efTZggP26TklwvH+3OxYv2efbS0vLX95vfzZ4tA+aeflp6dimVz+k4jxDx4IOyOugnn3h+jPh4uV++XBOHz/XuLY3xmjiUMkX/x/hZ2bIylVCZMp4fY/lyWf6iRQtpR9m0yXfxKaWUJ7TBPACMzhruyooVkjgiI2WtkdOnge3bpUSjlFLBoCWPALFY3C+M58qSJTKnFiBjSvbsAT76yGehKaWUaZo8AmTKFFm76K+/zL/32mvtje0dOshM5KNGGV9ZVSmlfC2gyYOI2hPRTiLaQ0TZ5qQlotuI6E8iSieinlle60tEu623voGL2jd69ZLGbjNLYwOydMWsWZm3vfce8O+/wIgRvotPKaXMCFjyIKJwAJMAdABQB8B9RJR1JZ+DAPoBmJXlvaUBjADQBEBjACOIKFfNqV2+PNC1qyxRYete747FIkt+Z50PsE4dGeN25YpOi6KUCo5AljwaA9jDzPuYOQ3AbABdHXdg5gPMvBWAJct72wFYwcynmfkMgBUA2gciaF8aMECmL1mwwNj+SUkyU/idd2Z/beJEmd1XBwcrpYIhkMkjGsAhh+cp1m3+fm/IuOMOIDYWmD/f2P4rVsi9s+RhG5awebPMnaWUUoEUyK66zn4jG610MfReIhoAYAAAVPFmHWo/CQuTMRtG56FbsULW7Yl2kSYtFplRIzxc5tfSUohSKlACWfJIAXCtw/PKAIwu3Wbovcw8jZnjmDmuXLlyHgfqT9WrG1sEj1kaxdu1c71PWJjMebVtm2e9uJRSylOBTB5/ALieiGKJqCCA3gAWGXzvMgBtiaiUtaG8rXVbrvTtt8BNN8k8Va4QSXXUe+/lfKyu1lYjo+0oSinlCwFLHsycDuBJyJf+XwDmMPN2IhpFRF0AgIhuJqIUAL0ATCWi7db3ngbwJiQB/QFglHVbrlSypLRVTJ7seh9bLyp3Uy5VqiQzieuSuEqpQNJZdYOAGejcGfjlF2mrqF49+z633QY0awa88477440ZI7cDByQxKaWUp3RW3RBGBEydKnNTPfKINHw7OnECWLvWPg27O08+KV16NXEopQJFk0eQREfLWI21a6UE4mjlSrl31kXXmagonSRRKRVYmjyCqG9fmSyxTZvM25cvl1UDGzUyfqzly4H69YEzZ3wbo1JKOaPJI4iIgMaN5fGOHVJ9xSzjO+64Q8ZvGFWypIxIzzqViVJK+YMmjxCwebOUGiZPlrXQH31UViA0Iy5Oel4FustuejqQkRHYc+YHx47JGB6dOVmFKk0eIaBBA2nfGDYMSEmRNTu6dDF3jLAwWShq6VLg8mW/hJnJ2rVAQgJQrhywZo3/z5ffnD4t0/i/+WawI1HKOU0eIYAImDZNRp7feKOMLPdEt27ApUv2ObH8Ze5c6Ur8229yviVL/Hu+/GbKFOk917+/PN6zJ9gRKZWdJo8Qce21slrgv/8CDz3k2TFatQIeeEBKA/6ydSvQrx/QtCkwcKAkkcWL/Xe+/Ob4ceDZZ4Gvv5YSaKFCwEsvBTsqpbLT5BFCHnlElpcdN86z90dEADNnyhe7P5w+LaWbEiVkZuBChWRlwx07gORk/5wzv/nwQ1mn5bnngGuuAV54AZg3D1i/PtiRKZWZJo8QQgQMHgxUq+bdcfbuBQ4e9E1MNswyg+/hw8B33wEVK8r2jh3lXquuvPfvv8CkSZKga9aUbc89BzzxhHSGUCqUaPLIYy5dAm64AZgwwbfHJZLqlE8/lbXYbWrWlPXU/VXayU8+/VTG6QwbZt8WFSW98EJwhQGVz2nyyGOKFJFBhwsXOl+ilhn4+29zy9cePy73d94pbSqOiIDXXpOG/tzq9GnjSwP7U2Qk0LOnTHSZ1bZtwGOPSVdupUKBJo886O67ZZLErVvt2ywWSSiNGskCU0armU6ckP2/+sr1PunpwOrVwM6d3kQdWLbk+ccf0sHgvvuCGw8APP649GRzZv9+YPp0uSkVCjR55EGdO8u4jwULJGnMnStjSe6+Gzh/Hhgxwt5W4c6rr8p7cpoq5coVWbRq2jTfxO8vR44AH3wgPcRsPZgaNpS/xYIFkkiCgRn44QdJwq506iRxv/EGcOGC8WNfvAi8+64knYsXvQ5VKTtmzpO3Ro0acX7WogVzw4bMFy4wlynDXKsW88yZzFev2vdJScn5GJs2MRMxDx3q/nxt2zLXru1dzP6UnMwcFcUMMNerxzx5sv218+flb9ShQ3BiW7xY4vrmm5z327BB9nv9dWPHjY9nvu46eQ/AXKIE89693ser8jYACWzgOzboX/L+uuX35LF9O/PJk/J4xw7m9PTMry9ZwlygAPOaNc7fb7Ew33orc/nyzGfPuj/fhAnyadq3z/U+Z88yP/QQ859/GrsGXxowgLlgQdfnHjtW4l+3LrBxMTO3asVcuTLzlSvu9733XuawMOZDh3Le78wZ5mLFmGNjmVetYl67lvm55+TflZl50iTm2bONnVPlL5o88nnycOfiReaqVZnr1mVOS8v+emKifNl+9pmx4+3cKZ+mSZNc7/PUU7JP7drMqakehe2Rq1eZ4+KYn3zS9T7//svcsSPz+vWBi4vZXpp47z1j+x85wjxkiD0JTJwoJRbb3zMpyf5afLyUPLOyWJgbNJDz3nWX99eg8hZNHpo83Fq4UD4B48c7f33/fuaMDGPHsliYq1Vj7tnT+etXrzI3ayalGYD5jTc8CtljGRmSMEOJxSJf3iVLStWZWRkZzPXry9+zTBnmbt2kmvHrr42996WX5L1JSebPrfIuo8lDG8zzsS5dpLF4xAhpTLbZvVvuY2Lcr6FuQyRzan3zjfPXCxQA4uOBn38G7r8f+PHHnBuIfeXoURk7ERYm3ZjdOXFCxlVwAFZnPnNGelG98gpQrJj594eFyYzMy5cDt98uf9vHHzc2qWZYmAxAjIyUgYlKmWYkw+TGm5Y8jNmzR+rG58yR5zt3MkdESHWIr6xezXzsmP35uXPOq8qccWzg90SfPsyVKhmv2//wQ/k1vnKl89dt7T/JyczDhmVvSzLr4kXjpTt/ePhh5goVjP97qLwPWvJQRlx3HXDoENCrlzwfOhQoXBjo3duz473ySua5uU6eBLp3lxlibYoXl3m4zp4FVq1yfhxm4OWXga5dZWDc5ctAaqq5WP76C5g1SwY2Gl2mt39/WSL49dezlz5++klKY2vWSNfasWNl6hCzpZTz54EXX5TpSIoUMV6684fRo2V8TkRE8GJQuZSRDJMbb1ryMO+559hU460z7dox16xpf96/P3N4uPN69QcekO6zycmZt6elMfftK7E89piUPu69l7l5c+YTJ4zHcs89cnwz72GWbrwA89Kl9m2ffirX0aiRvRT18suy3zPP2Bup3bFYJK7w8OD07HLFYjF+DcFw9izzuHGetQ0pc6AN5po8zFi/Xj4NERHedd98/305zt69zL//Lo+ff975vvv3MxctKr2cbF9cFy5IAgKYR42yb589m7lQIRm3sHOn+zi2bpVjvPyy+Wu4coW5ShXmxo3l/G+9Jce6887MX14WC/PTT8trr71m7NhTpsj+77xjPi5/2bNHGt6XLfPs/Xv3SmJ31e3bF554Qv5uw4b57xxKaPLQ5GGKxSJfbN72vNm1Sz5VEydKd9DoaOfdRW0mTpT9Z82S53fdJb/KP/kk+77r1jGXK8dcqpS0o+Tk7beZixdnPnXKs+uYPp25Rw/mr76S+Pr0cZ5UMzKYH31UBmReupTzMf/8UxJg+/bBbefIKjVVxvN07mz+vRcvSkkTYG7a1D+ll/R0GcBZvDhzZCTzwYO+P4ey0+ShySNoqleXX6K9e9sb4l1JT5df+GXLSvVSQgLzDz+43n/vXhktf8017rve/vOP+dizysiQcRQ5fdmnp0snANtjZ1+gFgvzzTdLMj1+3Pu4fO3VV6Wbb06DPF2ZONFezRgf7/PQmFn+frt2ydijhx/2zzmUMJo8SPbNe+Li4jghISHYYeRLI0bIuh9GJ/FLSpLG8alTja1bcfasrFnSqJE8vuUWmRK+aVPg1luB0qWDs/7F5cvSwB8fL5Mt2m633ip/k/37pQPBzTcHPjZ3UlKkM8CzzwL/+5/7/S0WWQAsNlaeX7oEVK0K3HEHMHu27+JatUrWt6laVZ5/+CFQo4bMpab8g4g2MXOc2/00eajcbP9+YMgQ4Pff5YvZ5scfgbvuCmwsqamyJseBAzJe5MQJienGG0N/0khAetytXCmJxN2YmOHDZTzMli32BLJxo6wlU7Sob+I5d07Wi6lTB/jlF98cU7lnNHkUCEQwSvlLbCywaJF0l92zR5LI/v2ynnugRUbKSpC51bBhMsCwgJtvhY8/tndTjomxb2/cWO6ZZdCot0aOlLVkfvwx8/Z//5UFyGwzDefkzBlZNjmY3aHzKi15KKUM++knSTAdOsj6MFkTze+/AwMGyH7erH64bZssI/Doo1Kd6ejyZam6qlRJ1nbPmqhOnpTZC4oUkYW1brsNmDJFE4hRRkse+udUSv3n4kXgvfeATZvs22yrLJ46BfToIWugzJ7tvIQSHS0rVXqzDDIz8NRTMph09OjsrxcuLCWPjRuBefMyv3boENCihawDX6yYDFCdNk1WYbRYPI9JZafJQyn1H2b5Yh4yREbmx8TIFy8gHRF69ZJqpKgo5++vUkVWZfzkE1ne1xNXrkhbxzvvAGXLOt/noYeAunWlo0Vammz7+2+gWTOZp+1//5MSyVtvyWwBn30GPPIIkJHhWUwqO00eSqn/REVJtdNvv8lEizffDLRpI68RATNnAtdck/MxXnxRSjCTJ3sWQ2SktKs8/rjrfcLDgTFjpJ1r2jRZBbJ5c0k8a9bY20KIpO1k5EhgxgxZiVH5hrZ5KKUyuXpVZiOuXNnzhu9OnaRaKTlZqpmMmjxZumA3aeJ+X2bgtdekpPPww9K7bcUKoHp15/tPnSrVWRUqGI8nP9Kuupo8lAqahAQZi9Ojh/veWzZJScBNN0mV1KefmjvfoUNSGjEyvufqVSnZDB6sjejOhGSDORG1J6KdRLSHiIY7eb0QEX1rfX0DEcVYt8cQ0WUiSrTePg5k3Eopc+LigHvvNZ44LBappipRQroBm3XttcYHhv7wA/D00zIDtC+cO+eb4/jKmjWBWY8mYMmDiMIBTALQAUAdAPcRUZ0suz0K4AwzVwcwAYDjx2gvMzew3gYGJGillMeuXJHeUgsWuN93+nTp5jtunOtGcl+5+25JVGPGSEO6WcwygwIAzJ0ryxps3uzbGB0lJxtv6B8/XsY4zZ/vv3j+Y2QOE1/cADQFsMzh+UsAXsqyzzIATa2PCwA4CYAAxADYZuZ8OreVUsGVkSGTYxYrlvPa8MeOyVK8LVsGblr4tDTmtm2ZCxRg/vln4+/bvVtmfa5USeYzO3BAZmAuU4Z52zbfx7lkicwZ1qGDfSEyVz79VPbt2dO7RcoQgotBRQM45PA8xbrN6T7MnA7gHIAy1tdiiWgzEa0hohb+DlYp5Z2wMOnWW768zEXlOHbEUcmSMrp9yhTfjEw3IiICmDMHqFVLGtttY1lcSU2Vnlp16wLr1km8RYrInFsrV8piY3fcAeza5bsYz52TbtKVKklHgKZNgX/+cb7v5cvSo6xtW+Crr6T9x98COT2Js49F1po5V/v8A6AKM58iokYAFhLRDcx8PtObiQYAGAAAVbwZ3qqU8onoaJmXqmVL+WJbtQqoXz/zPgULylxZgVaihCS3c+eAQoVc73funDTk79snPbveew+oWNH+evXqkkBatgRat5b5vsqUcX08o5KT5W/zzTfS9fmjj1xX6RUuDKxdK+fN6Vp8KZAljxQA1zo8rwzgiKt9iKgAgBIATjPzFWY+BQDMvAnAXgA1sp6Amacxcxwzx5UrV84Pl6CUMqtKFUkgJUrIuAyb1FTg9tuBJUuCF1vVqvZk9s039uWO5861j5IvUUISw88/y7LGjonDpnZtef3pp2UwpS/Ury9LBDduLH+n+fOlxHTiBPD117LP+vUyrsZikb+zryalNMRI3ZYvbpBSzj4AsQAKAtgC4IYs+wwG8LH1cW8Ac6yPywEItz6uBuAwgNI5nU/bPJQKLampmR+PGCF19J6uYOhLmzdLLA0bSvsLwFyjhudtB1u3Mi9cyDxjhqwyed99soyuzZEjrt97/rwsZub493I0fLjE9/DDsjDaddd5vuiZMwjFxaAAdASwC1JyeMW6bRSALtbHkQDmAtgDYCOAatbtPQBstyacPwF0dncuTR5KhaYFC5irVpWFnXr3DnY0du+9J8si338/8/LlnicO2zLG0i+LOSyMOTbWviTyN98wFynCPH++8/c/8YQszLVxo/PXr161L39cqZJnC3jlxGjy0EGCSqmA2rpVqmEyMmQ+KnfTnQQS+2g6+b175dquv17mBytY0P7a0aMy0n3DBpl76+WX7edctUraTZ59VtpWcrJkiVSXOU6L7ws6wlyTh1IhKzlZGoHrZB3plU+kpgL9+0vbRZ8+Ms4lIwOoV096Sm3Z4n5BLn/RxaCUUiHLtqxsfhUZKZNM1qkDvPqqjMZfuVJWoVyzJniJwwxNHkopFQREUmXVrZskkZgYubXIJaPYNHkopVQQ2aru6tWTW26hc0oqpZQyTZOHUkop0zR5KKWUMk2Th1JKKdM0eSillDJNk4dSSinTNHkopZQyTZOHUkop0zR5KKWUMi3PToxIRCcAJHtxiLKQNdTzG73u/EWvO38xct1Vmdntanp5Nnl4i4gSjMwsmdfodecvet35iy+vW6utlFJKmabJQymllGmaPFybFuwAgkSvO3/R685ffHbd2uahlFLKNC15KKWUMk2TRxZE1J6IdhLRHiIaHux4/ImIPiOi40S0zWFbaSJaQUS7rfelghmjrxHRtUS0ioj+IqLtRDTEuj2vX3ckEW0koi3W6x5p3R5LRBus1/0tERUMdqz+QEThRLSZiH60Ps8v132AiJKIKJGIEqzbfPJZ1+ThgIjCAUwC0AFAHQD3EVGd4EblV18AaJ9l23AAK5n5egArrc/zknQAzzFzbQC3ABhs/TfO69d9BUBrZr4RQAMA7YnoFgBjAUywXvcZAI8GMUZ/GgLgL4fn+eW6AeB2Zm7g0EXXJ591TR6ZNQawh5n3MXMagNkAugY5Jr9h5ngAp7Ns7gpghvXxDADdAhqUnzHzP8z8p/XxBcgXSjTy/nUzM/9rfRphvTGA1gDmWbfnuesGACKqDOAuANOtzwn54Lpz4JPPuiaPzKIBHHJ4nmLdlp9UYOZ/APmiBVA+yPH4DRHFAGgIYAPywXVbq24SARwHsALAXgBnmTndukte/bxPBPAiAIv1eRnkj+sG5AfCciLaREQDrNt88lkv4KMA8wpysk27o+VBRBQFYD6AZ5j5vPwYzduYOQNAAyIqCWABgNrOdgtsVP5FRJ0AHGfmTUTUyrbZya556rodNGPmI0RUHsAKIvrbVwfWkkdmKQCudXheGcCRIMUSLMeIqCIAWO+PBzkenyOiCEji+JqZv7NuzvPXbcPMZwGshrT5lCQi24/IvPh5bwagCxEdgFRDt4aURPL6dQMAmPmI9f445AdDY/jos67JI7M/AFxv7YlREEBvAIuCHFOgLQLQ1/q4L4DvgxiLz1nruz8F8Bczj3d4Ka9fdzlriQNEVBhAG0h7zyoAPa275bnrZuaXmLkyM8dA/j//wsx9kMevGwCIqCgRFbM9BtAWwDb46LOugwSzIKKOkF8m4QA+Y+bRQQ7Jb4joGwCtIDNtHgMwAsBCAHMAVAFwEEAvZs7aqJ5rEVFzAGsBJMFeB/4ypN0jL193fUjjaDjkR+McZh5FRNUgv8hLA9gM4AFmvhK8SP3HWm31PDN3yg/Xbb3GBdanBQDMYubRRFQGPvisa/JQSillmlZbKaWUMk2Th1JKKdM0eSillDJNk4dSSinTNHkopZQyTZOHUrkAEcUQERNRvlt3W4UmTR5KKaVM0+ShlFLKNE0eShlA4kUi2ktEl60L7Dxgfc1WpXQ/Ef1KRKlE9DcRtc1yjNusCxClEtExIprguAiR9RzPWRfpuUJEKUT0TpZQqloX8LlERDuI6M4AXL5S2WjyUMqYtyALBg2GLBT2DoCpRHSXwz7/A/ABZLGlFQC+J6JoALDeL4FMhdHQeqz7rMexeYDh1JwAAAH4SURBVBvAa9ZtNwDohcxLBADAaOs5boTMxTbbOkOwUgGl05Mo5YZ1UrmTANoy81qH7RMB1AAwCMB+AK/a5kIjojAAf0PmkHqViEYDuBdADWa2WPfpB2AqgFKQH3InIVPEf+wkhhjrOQYy81TrtmjITNAtmPlX31+5Uq7peh5KuVcHQCSApUTk+GsrAsABh+e/2x4ws4WINljfC8jaGb/bEofVrwAKAqhuPX4hyLKgOdnq8Ng2jXieW7hKhT5NHkq5Z6ve7QyZhdTRVThfXCgrgusFh9jgMWznkzcxs3URK61+VgGnHzql3NsB4AqAqsy8J8st2WG/W2wPrOuGNIasmWE7RlNrdZZNcwBpkOVgbee4w4/XoZTPaMlDKTeY+QIRjQMwzpoU4gFEQZKFBcBy665PENEuyFohgwBUBTDF+tpkAM8AmExE7wOoBmAMgI+Y+RIAWLe/Q0RXrOcoA6ARM9uOoVTI0OShlDGvQRbMeh6SEM4DSIT0sLIZDuBZADcBSAZwNzOnAAAzHyaiDgDetb7vLIBZkIWobF4CcMZ6rsrW833pv0tSynPa20opLzn0hLqZmROCG41SgaFtHkoppUzT5KGUUso0rbZSSillmpY8lFJKmabJQymllGmaPJRSSpmmyUMppZRpmjyUUkqZpslDKaWUaf8HJZQLLj/vNJgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best RMSE: 103\n"
     ]
    }
   ],
   "source": [
    "args = get_args_parser()\n",
    "args, unknown = args.parse_known_args()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\" -- GPU is available -- \")\n",
    "\n",
    "trn_set = Predictor1_Dataset(train=True, last_padding=False)\n",
    "trn_loader = DataLoader(trn_set, batch_size=92, num_workers=0, drop_last=False, shuffle=False)\n",
    "val_set = Predictor1_Dataset(train=False, last_padding=False)\n",
    "val_loader = DataLoader(val_set, batch_size=23, num_workers=0, drop_last=False, shuffle=False)\n",
    "trn_set_padding = Predictor1_Dataset(train=True, last_padding=True)\n",
    "trn_loader_padding = DataLoader(trn_set_padding, batch_size=args.batch_size, num_workers=0, drop_last=False, shuffle=True)\n",
    "print(len(trn_set), len(val_set))\n",
    "\n",
    "model = Predictor_1(8, 1, 0.25).apply(init_weights).cuda()\n",
    "if args.finetune:\n",
    "    model.load_state_dict(torch.load(args.load_checkpoint))\n",
    "summary(model, (8, 100)) # architecture visualization\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=args.lr, amsgrad=True, weight_decay=args.weight_decay)\n",
    "criterion = nn.L1Loss()\n",
    "loss_function  = nn.MSELoss()\n",
    "\n",
    "best_rmse = 1000\n",
    "trn_loss_record, val_loss_record = [], []\n",
    "for epoch in range(args.epochs):\n",
    "    model.train()\n",
    "    batch = 0\n",
    "    n_minibatch = (len(trn_set_padding)//args.batch_size)\n",
    "    if args.lr_schedule:\n",
    "        adjust_learning_rate(optimizer, args.epochs, epoch+1, args.warm_up, args.lr, args.min_lr)\n",
    "    for inputs, targets in trn_loader_padding:\n",
    "        batch += 1\n",
    "        optimizer.zero_grad()\n",
    "        output = model(inputs.cuda().float())\n",
    "        loss = criterion(output, targets.reshape(-1, 2).cuda().float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%50==1:\n",
    "            print('epoch:[%d / %d] batch:[%d / %d] loss= %.3f lr= %.2e' % \n",
    "                (epoch + 1, args.epochs, batch, n_minibatch, loss.mean(), optimizer.param_groups[0][\"lr\"]))\n",
    "\n",
    "    # model evaluation per epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        trn_loss, val_loss = 0, 0\n",
    "        for inputs, targets in trn_loader:\n",
    "            output = model(inputs.cuda().float())\n",
    "            loss = loss_function(output[:, 0] , targets[:, 0].cuda().float())\n",
    "            trn_loss += loss.mean()\n",
    "        for inputs, targets in val_loader:\n",
    "            output = model(inputs.cuda().float())\n",
    "            loss = loss_function(output[:, 0] , targets[:, 0].cuda().float())\n",
    "            val_loss += loss.mean()\n",
    "        trn_loss_record.append(trn_loss.cpu())\n",
    "        val_loss_record.append(val_loss.cpu())\n",
    "        print('trn_loss: %.3f, val_loss: %.3f' % ((trn_loss), (val_loss)))\n",
    "\n",
    "    # inverse transform to real EOL\n",
    "    trn_rmse, test_rmse = predictor1_model_evaluation(model, best_rmse, eval_length=[0, 19, 99])\n",
    "    print('training set RMSE 1 cycle: %d, 20 cycle: %d, 100 cycle: %d' %\n",
    "            (trn_rmse[0], trn_rmse[1], trn_rmse[2]))\n",
    "    print('testing set RMSE 1 cycle: %d, 20 cycle: %d, 100 cycle: %d' %\n",
    "            (test_rmse[0], test_rmse[1], test_rmse[2]))\n",
    "\n",
    "    # save best testing loss      \n",
    "    if test_rmse[2]<best_rmse:\n",
    "        best_rmse = test_rmse[2]\n",
    "        if args.finetune:\n",
    "            torch.save(model.state_dict(), 'predictor1_finetuned.pth')\n",
    "        else:\n",
    "            torch.save(model.state_dict(), 'predictor1_best_model.pth')\n",
    "\n",
    "# training finished\n",
    "loss_profile(trn_loss_record, val_loss_record)\n",
    "print('best RMSE: %d' % (best_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
